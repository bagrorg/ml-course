{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G40l9lO2ObLr"
   },
   "source": [
    "## Домашнее задание №2, теоретическое. Логические методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решите предложенные задачи. Каждая задача должна быть подробно обоснована, задачи без\n",
    "обоснования не засчитываются. Решения пишутся в свободной форме, однако так, чтобы проверяющий смог разобраться. Если проверяющий не смог разобраться в решении какой-нибудь задачи, то\n",
    "она автоматически не засчитывается. Если используются какие-либо внешние источники, их нужно обязательно указывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C86UTDuUlQBk"
   },
   "source": [
    " ### Задача 1 (1 балл). Кроссвалидация, LOO, k-fold.\n",
    " \n",
    " Объясните, стоит ли использовать оценку `leave-one-out-CV` или `k-fold-CV` с небольшим $k$ в случае, когда:\n",
    "\n",
    " * обучающая выборка содержит очень малое количество объектов;\n",
    " * обучающая выборка содержит очень большое количество объектов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX6sPkky9jS6"
   },
   "source": [
    "### Задача 2 (1 балл). Логистическая регрессия, вывод функции потерь.\n",
    "\n",
    "Рассмотрим выборку объектов $X = \\{x_1,...,x_l\\}$ и их целевых меток $Y = \\{y_1,...,y_l\\}$, где $y_i ∈ \\{0, 1\\}$. Предположим, что мы хотим обучить линейный классификатор:\n",
    "\n",
    "$$Q(w, X^l) = \\sum\\limits_{i=1}^l\\mathcal{L}(y_i<w,x_i>) \\rightarrow \\min\\limits_w$$\n",
    "\n",
    "где $w$ – веса линейной модели, $\\mathcal{L}(y, z)$ – некоторая гладкая функция потерь.\n",
    "\n",
    "Так как решается задача двухклассовой классификации, то будем обучать классификатор предсказывать вероятности принадлежности объекта классу $1$, то есть решать задачу логистической регрессии. Для измерения качества такого классификатора обычно используют правдоподобие $P (Y|X )$ целевых меток $Y$ при заданных объектах $X$ в соответствии с предсказанными распределениями $p$ — чем выше правдоподобие, тем точнее классификатор. Для удобства с вычислительной точки зрения обычно используется отрицательный логарифм правдоподобия, также называемый LogLoss (Logarithmic Loss). Будем считать, что пары объект-ответ $(x_i,y_i)$ независимы между собой для разных $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UX9noDn-jcv"
   },
   "source": [
    "#### 1. (0.5 балла) \n",
    "Покажите что:\n",
    "\n",
    "$$\\text{LogLoss} =  -\\text{LogLikelihood} = - \\log(P(Y|X)) = - \\sum\\limits_{i=1}^l (y_i \\log \\tilde y_i + (1-y_i) \\log (1-\\tilde y_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAZJvhZVA5-7"
   },
   "source": [
    "#### 2. (0.5 балла) \n",
    "Для того, чтобы классификатор возвращал числа из отрезка $[0,1]$, положите\n",
    "$$\n",
    "p(y_i=1|x_i) = \\sigma \\left ( \\left< w, x_i \\right> \\right) =\n",
    "\\frac{1}{1 + \\exp{\\left(- \\left< w, x_i \\right> \\right)}};\n",
    "$$\n",
    "\n",
    "сигмоидная функция монотонно возрастает, поэтому чем больше скалярное призведение, тем большая вероятность положительного класса будет предсказана объекту.\n",
    "\n",
    "Подставьте трансформированный ответ линейной модели в логарифм правдоподобия. К какой функции потерь мы пришли? (Обратите внимание, что функция обычно записывается для классов $\\{-1, 1\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG4d_zxADKDF"
   },
   "source": [
    "### Задача 3 (1.5 балла). Логистическая регрессия, решение оптимизационной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA5GvcRcDY52"
   },
   "source": [
    "#### 1. (0.8 балла)\n",
    "\n",
    "Докажите, что в случае линейно разделимой выборки не существует вектора параметров (весов), который бы максимизировал правдоподобие вероятностной модели логистической регрессии в задаче двухклассовой классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBN5jiVE32G"
   },
   "source": [
    "#### 2. (0.4 балла)\n",
    "\n",
    "Предложите, как можно модифицировать вероятностную модель, чтобы оптимум достигался."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C4UiKK3HV94"
   },
   "source": [
    "#### 3. (0.3 балла)\n",
    "Выпишите формулы пересчета значений параметров при оптимизации методом градиентного спуска для обычной модели логистической регрессии и предложенной модификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtrnMSYXKHRS"
   },
   "source": [
    "### Задача 4 (1 балл). Мультиномиальная регрессия\n",
    "\n",
    "В случае многоклассовой классификации логистическую регрессию можно обобщить: пусть для каждого класса $k$ есть свой вектор весов $w_k$. Тогда вероятность принадлежности классу $k$ запишем следующим образом:\n",
    "\n",
    "$$\n",
    "P(y=k | x, W) = \\frac{e^{\\langle w_k, x\\rangle}}{\\sum_{j=1}^K e^{\\langle w_j, x\\rangle}}\n",
    "$$\n",
    "\n",
    "Тогда оптимизируемая функция примет вид:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{sm}(W) = -\\sum_{i=1}^N \\sum_{k=1}^K [y_i=k]\\ln P(y_i=k | x_i, W),~\\text{где}~[y_i=k]=\\begin{cases}\n",
    "1, y_i=k,\\\\\n",
    "0, \\text{иначе}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Пусть количество классов $K=2$. Для простоты положим, что выборка линейно неразделима."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_Rd-dJWKm8r"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    " Единственно ли решение задачи? Почему? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXe4eM0JL0xL"
   },
   "source": [
    "#### 2. (0.5 балла)\n",
    " Покажите, что предсказанные распределения вероятностей на классах в случае логистической и мультиномиальной регрессий будут совпадать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2a3lm4KMFGl"
   },
   "source": [
    "### Задача 5 (1.5 балла) Решающие деревья, константное предсказание, функции потерь.\n",
    "\n",
    "Допустим, при построении решающего дерева в некоторый лист попало $N$ объектов $x_1, ... , x_N$ с метками $y_1, ... , y_N$.\n",
    "Предсказание в каждом листе дерева - константа. Найдите, какое значение $\\tilde y$\n",
    "должен предсказывать этот лист для минимизации следующих функций потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBcO-ZNpMSYY"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    "Mean Squared Error (средний квадрат ошибки) для задачи регрессии:\n",
    "        \\begin{equation}Q=\\frac{1}{N}\\sum_{i=1}^N (y_i - \\tilde y)^2;\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrKaxpXmNaMf"
   },
   "source": [
    "#### 2. (0.5 балла) \n",
    " Mean Absolute Error (средний модуль отклонения) для задачи регрессии:\n",
    "        \\begin{equation}Q=\\frac{1}{N}\\sum_{i=1}^N |y_i - \\tilde y|.\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnKGc3keOgF4"
   },
   "source": [
    "#### 3. (0.5 балла)\n",
    " $\\text{LogLoss}$ (логарифмические потери) для задачи классификации:\n",
    "        $$Q=-\\frac{1}{N}\\sum_{i=1}^N \\left(y_i\\log\\tilde y+(1-y_i)\\log(1-\\tilde y)\\right),\n",
    "            \\quad \\tilde y\\in[0,1], \\quad y_i \\in \\{0,1\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSmKQAILP0T0"
   },
   "source": [
    " ### Задача 6 (1 балл). Решающие деревья, функции потерь, impurity functions.\n",
    "$$\n",
    " \\Phi(U) - \\frac{|U_1|}{|U|}\\Phi(U_1) - \\frac{|U_2|}{|U|}\\Phi(U_2) \\to \\max\n",
    "$$\n",
    "таким выражением в лекции задается критерий, по которому происходит ветвление вершины  решающего дерева. Давайте разберемся подробнее. \n",
    "\n",
    "Impurity function $\\Phi(U)$ («функция неопределенности» или «функция нечистоты») используется для того, чтобы измерить степень неоднородности целевых меток $y_1,\\dots, y_l$ для множества объектов $U$ размера $l$. Например, при обучении решающего дерева в текущем листе  выбирается такое разбиение множества объектов $U$ на два непересекающихся множества $U_1$ и $U_2$, чтобы impurity function $\\Phi(U)$ исходного множества $U$ как можно сильнее превосходила нормированную impurity function в новых листьях $\\frac{|U_1|}{|U|}\\Phi(U_1) + \\frac{|U_2|}{|U|}\\Phi(U_2)$. Отсюда и получается, что нужно выбрать разбиение, решающее  задачу\n",
    "$$\n",
    " \\Phi(U) - \\frac{|U_1|}{|U|}\\Phi(U_1) - \\frac{|U_2|}{|U|}\\Phi(U_2) \\to \\max.\n",
    "$$\n",
    "Полученную разность называют Gain (выигрыш), и она показывает, на сколько удалось уменьшить «неопределенность» от разбиения листа два новых. \n",
    "\n",
    "В соответствии с одним из возможных определений, impurity function — это значение функционала ошибки $Q = \\frac{1}{l}\\sum\\limits_{i=1}^l \\mathcal{L}(y_i, \\tilde{y})$ в листе с множеством объектов $U$ при константном предсказании $\\tilde{y}$, оптимальном для $Q$ (см. задачу 7):\n",
    "    $$\n",
    "        \\Phi(U) = \\frac{1}{l}\\sum\\limits_{i=1}^l \\mathcal L (y_i, \\tilde y).\n",
    "    $$\n",
    "Понятно, что каждому критерию разбиения соответствует своя impurity function $\\Phi(U)$, а в основе каждой $\\Phi(U)$ лежит некоторая функция потерь. Давайте разберемся, откуда берутся различные критерии разбиения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZNypaDVQDYZ"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    "Покажите, что для квадратичных потерь $\\mathcal L (y_i, \\tilde y) = (y_i - \\tilde y)^2$ в задаче регрессии $y_i \\in \\mathbb{R}$ impurity function $\\Phi(U)$ равна выборочной дисперсии целевых меток объектов, попавших в лист дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeWp8iIxR96A"
   },
   "source": [
    "#### 2. (0.5 балла)\n",
    " Покажите, что для функции потерь $\\text{Logloss}$  $\\mathcal L (y_i, \\tilde y) =-y_i\\log(\\tilde y) - (1-y_i)\\log(1 - \\tilde y)$ в задаче классификации $y_i \\in \\{0,1\\}$ impurity function $\\Phi(U)$ соответствует энтропийному критерию разбиения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvAw2PMSTrkw"
   },
   "source": [
    "### Задача 7 (1 балл). Решающие деревья, индекс Джини\n",
    "\n",
    "Пусть имеется построенное решающее дерево для задачи многоклассовой классификации. Рассмотрим лист дерева с номером $m$ и объекты $R_m$, попавшие в него. Обозначим за $p_{mk}$ долю объектов $k$-го класса в листе $m$. *Индексом Джини* этого листа называется величина\n",
    "$$\\sum_{k = 1}^{K} p_{mk} (1 - p_{mk}),$$\n",
    "где $K$ — общее количество классов. Индекс Джини обычно служит мерой того, насколько хорошо в данном листе выделен какой-то один класс (см. impurity function в предыдущей задаче)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39o2PqSqT9U-"
   },
   "source": [
    "#### 1. (0.5 балла)\n",
    "Поставим в соответствие листу $m$ алгоритм классификации $a(x)$, который предсказывает класс случайно, причем класс $k$ выбирается с вероятностью $p_{mk}$. Покажите, что матожидание частоты ошибок этого алгоритма на объектах из $R_m$ равно индексу Джини. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2_WiDXWU5Mu"
   },
   "source": [
    "#### 2. (0.5 балла)\n",
    "*Дисперсией класса $k$* назовем дисперсию выборки $\\{ [y_i = k]:\\ x_i \\in R_m$\\},\n",
    "    где $y_i$ - класс объекта $x_i$, $[f]$ — индикатор истинности выражения $f$, равный 1 если $f$ верно, и нулю в противном случае, а $R_m$ — множество объектов в листе.\n",
    "    Покажите, что сумма дисперсий всех классов в заданном листе равна его индексу Джини."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6-ynDEVYCF4"
   },
   "source": [
    "### Задача 8 (2 балла). Бинарные решающие деревья, MSE\n",
    "\n",
    "Предложите алгоритм построения *оптимального* бинарного решающего дерева для задачи регрессии на $l$ объектах в $n$-мерном пространстве с асимптотической сложностью $O(n l \\log l)$. В качестве предикатов нужно рассматривать пороговые правила (наиболее распространенный случай на практике). Для простоты можно считать, что  получающееся дерево близко к сбалансированному (т.е. его глубина имеет порядок $O(\\log l)$) и в качестве функции ошибки  используется Mean Squared Error (MSE):\n",
    "\t$$Q=\\frac{1}{l}\\sum_{i=1}^l (y_i - \\tilde y_i)^2.$$\n",
    "Под оптимальностью в данной задаче подразумевается, что в каждом узле дерева делается оптимальное с точки зрения MSE разбиение на два поддерева.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Practice5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
