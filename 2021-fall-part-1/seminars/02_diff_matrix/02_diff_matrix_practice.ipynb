{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/logo_fmkn.png\" width=300 style=\"display: inline-block;\"></center> \n",
    "\n",
    "## Машинное обучение\n",
    "### Семинар 2. Матрично-векторное дифференцирование\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "9 сентября 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " * на лекции мы осознаем важность градиентного спуска\n",
    " * ещё умение считать производные (градиенты) пригодится для понимания и написания нейронных сетей (обратное распространение ошибки)\n",
    "\n",
    "<center><img src=\"./images/feed-forward-and-back-prop.png\" width=500 style=\"display: inline-block;\"></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Определения*\n",
    " * При отображении вектора в число $f(x): \\mathbb{R}^n \\to \\mathbb{R}$\n",
    " \n",
    " $\\nabla_x f(x) =\n",
    "        \\bigg[\n",
    "            \\frac{\\partial f}{\\partial x_1},\n",
    "            \\dots,\n",
    "            \\frac{\\partial f}{\\partial x_n}\n",
    "        \\bigg]^T\n",
    "$\n",
    " * При отображении матрицы в число $f(A): \\mathbb{R}^{n \\times m} \\to \\mathbb{R}$\n",
    "\n",
    "$ \\nabla_A f(A) =\n",
    "        \\bigg(\n",
    "            \\frac{\\partial f}{\\partial a_{ij}}\n",
    "        \\bigg)_{i,j=1}^{n,m}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задача 1\n",
    "\n",
    "Пусть $a \\in \\mathbb{R}^n$ — вектор параметров, а $x \\in \\mathbb{R}^n$ — вектор переменных. Найдите производную их скалярного произведения по вектору переменных $\\nabla_x a^Tx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Решение\n",
    "\n",
    "$ \\frac{\\partial}{\\partial x_i} a^Tx = \\frac{\\partial}{\\partial x_i}\\sum\\limits_j a_jx_j = a_i$, поэтому $\\nabla_x a^Tx = a$\n",
    "\n",
    "Заметим, что $a^Tx$ — это число, поэтому $a^Tx = x^Ta$, следовательно, $\\nabla_x x^Ta = a$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задача 2\n",
    "\n",
    "Пусть теперь $A \\in \\mathbb{R}^{n\\times n}$. Необходимо найти $\\nabla_x x^TAx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Решение\n",
    "\n",
    "$ \\frac{\\partial}{\\partial x_i} x^TAx = \\frac{\\partial}{\\partial x_i}\\sum\\limits_j x_j (Ax)_j = \\frac{\\partial}{\\partial x_i}\\sum\\limits_j x_j \\bigg(\\sum\\limits_k a_{jk}x_k\\bigg) = \\\\\n",
    "= \\frac{\\partial}{\\partial x_i}\\sum\\limits_{j,k} a_{jk} x_j x_k = \\\\\n",
    " = \\sum\\limits_{j \\neq i} a_{ji} x_j + \\sum\\limits_{k \\neq i} a_{ik} x_k + 2a_{ii}x_i = \\sum\\limits_{j} a_{ji} x_j + \\sum\\limits_{k} a_{ik} x_k = \\sum\\limits_{j} (a_{ji} + a_{ij}) x_j$\n",
    " \n",
    " Поэтому $\\nabla_x x^TAx = (A + A^T)x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задача 3\n",
    "\n",
    "Пусть $A \\in \\mathbb{R}^{n\\times n}$. Необходимо найти $\\nabla_A \\det A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Решение\n",
    "\n",
    "Воспользуемся следствием теоремы Лапласа о разложении определителя по строке:\n",
    "\n",
    "$\\frac{\\partial}{\\partial a_{ij}} \\det A = \\frac{\\partial}{\\partial a_{ij}}\\bigg[\\sum\\limits_k (-1)^{i+k}a_{ik}M_{ik}\\bigg] = (-1)^{i+j}M_{ij}, \\; $\n",
    "где $M_{ik}$ — дополнительный минор матрицы $A$. \n",
    "\n",
    "Также вспомним формулу для элементов обратной матрицы\n",
    "$(A^{-1})_{ij} = \\frac{1}{\\det A}(-1)^{i+j}M_{ji}.$\n",
    "    \n",
    "Подставляя выражение для дополнительного минора, получаем ответ $\\nabla_A \\det A = (\\det A) A^{-T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задача 4\n",
    "\n",
    "Пусть $A \\in \\mathbb{R}^{n \\times n},\\ B \\in \\mathbb{R}^{n \\times n}$. Необходимо найти $\\nabla_A \\text{tr}(AB)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Решение\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial a_{ij}} \\text{tr}(AB) = \\frac{\\partial}{\\partial a_{ij}} \\sum\\limits_k (AB)_{kk} = \\frac{\\partial}{\\partial a_{ij}} \\sum\\limits_{k,l} a_{kl}b_{lk} = b_{ji}.\n",
    "$\n",
    "\n",
    "$ \\text{То есть }\\nabla_A \\text{tr}(AB) = B^T.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задача 5\n",
    "\n",
    "Пусть $x \\in \\mathbb{R}^n, \\, A \\in \\mathbb{R}^{n \\times m}, \\, y \\in \\mathbb{R}^m.$ Необходимо найти $\\nabla_A x^TAy$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Решение\n",
    "\n",
    "Воспользовавшись выполняющимся для скаляра равенством $a = tr(a)$, циклическим свойством следа матрицы (для матриц подходящего размера): \n",
    "\n",
    "$ \\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)$\n",
    "\n",
    "и результатом предыдущей задачи, получаем\n",
    "\n",
    "$ \\nabla_A x^TAy = \\nabla_A \\text{tr} (x^TAy) =  \\nabla_A \\text{tr}(Ayx^T) = xy^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Наконец, научимся считать градиенты для сложных функций.\n",
    "\n",
    "Допустим, даны функции $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ и $g: \\mathbb{R}^m \\to \\mathbb{R}$.\n",
    "Тогда градиент их композиции можно вычислить как\n",
    "\n",
    "$ \\nabla_x g \\left( f(x) \\right) =\n",
    "    J_{f}^T (x)\n",
    "    \\nabla_z \\left. g(z) \\right|_{z = f(x)},\n",
    "$\n",
    "\n",
    "где $J_f (x) = \\left( \\frac{\\partial f_i(x)}{\\partial x_j}  \\right)_{i, j = 1}^{m, n}$ — матрица Якоби для функции $f$.\n",
    "\n",
    "Если $m = 1$ и функция $g(z)$ имеет всего один аргумент, то формула упрощается:\n",
    "$ \\nabla_x g \\left( f(x) \\right) =\n",
    "    g'(f(x))\n",
    "    \\nabla_x f(x).\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задача 6\n",
    "\n",
    "Вычислите градиент логистической функции потерь для линейной модели по параметрам этой модели:\n",
    "$ \\nabla_w\n",
    "        \\log \\left(\n",
    "            1 + \\exp(-y \\langle w, x \\rangle)\n",
    "        \\right)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Решение\n",
    "\n",
    "$ \\nabla_w\n",
    "        \\log \\left(\n",
    "            1 + \\exp(-y \\langle w, x \\rangle)\n",
    "        \\right)\n",
    "        = \\\\\n",
    "        =\n",
    "        \\frac{\n",
    "            1\n",
    "        }{\n",
    "            1\n",
    "            +\n",
    "            \\exp(-y \\langle w, x \\rangle)\n",
    "        }\n",
    "        \\nabla_w \\left(\n",
    "            1\n",
    "            +\n",
    "            \\exp(-y \\langle w, x \\rangle)\n",
    "        \\right)\n",
    "        =\\\\\n",
    "        =\n",
    "        \\frac{\n",
    "            1\n",
    "        }{\n",
    "            1\n",
    "            +\n",
    "            \\exp(-y \\langle w, x \\rangle)\n",
    "        }\n",
    "        \\exp(-y \\langle w, x \\rangle)\n",
    "        \\nabla_w \\left(\n",
    "            -y \\langle w, x \\rangle\n",
    "        \\right)\n",
    "        =\\\\\n",
    "        =\n",
    "        -\n",
    "        \\frac{\n",
    "            1\n",
    "        }{\n",
    "            1\n",
    "            +\n",
    "            \\exp(-y \\langle w, x \\rangle)\n",
    "        }\n",
    "        \\exp(-y \\langle w, x \\rangle)\n",
    "        y\n",
    "        x\n",
    "        =\\\\\n",
    "        =\n",
    "        \\left\\{\n",
    "            \\sigma(z)\n",
    "            =\n",
    "            \\frac{\n",
    "                1\n",
    "            }{\n",
    "                1 + \\exp(-z)\n",
    "            }\n",
    "        \\right\\}\n",
    "        =\\\\\n",
    "        =\n",
    "        -\n",
    "        \\sigma(-y \\langle w, x \\rangle)\n",
    "        y\n",
    "        x\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Многомерная линейная регрессия\n",
    "\n",
    "Задача найти $\\hat{w} = \\arg\\min\\limits_{w\\in R^n} \\frac{1}{l} \\sum\\limits_{i=1}^l (w^T x_i - y_i)^2$\n",
    "\n",
    "Для решения два подхода:\n",
    "1. Численно (градиентный спуск)\n",
    "2. Аналитически\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Перепишем минимизируемую функцию в матричном виде:\n",
    "\n",
    "$$\\sum\\limits_{i=1}^l (w^T x_i - y_i)^2 = (Xw - y)^T(Xw - y) = \\\\ w^TX^TXw - y^TXw - w^TX^Ty + y^Ty = w^TX^TXw - 2 y^TXw +y^2 $$ \n",
    "\n",
    "$(y^TXw = w^TX^Ty)$, т.к. транспонированный скаляр равен себе\n",
    "\n",
    "\n",
    "Замечание (что нам нужно знать про градиент):\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} w^Ta = a$ (проверятеся покоординатно)\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} a^Tw = a$ (проверятеся покоординатно)\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} w^Tw = 2w$ (проверяется покоординатно)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\frac{\\partial}{\\partial w} f(\\vec{g}(w)) = \\frac{\\partial(\\vec{g})}{\\partial(w)} (\\frac{\\partial}{\\partial w}f)(\\vec{g}(w))$, где $\\frac{\\partial(\\vec{g})}{\\partial(w)} g(w)$ — матрица производных $\\vec{g}(w)$, \n",
    "\n",
    "т.е. $\\begin{pmatrix}\n",
    "{\\partial g_1 \\over \\partial w_1}(w) & {\\partial g_1 \\over \\partial w_2}(w) & \\cdots & {\\partial g_1 \\over \\partial w_n}(w) \\\\\n",
    "{\\partial g_2 \\over \\partial w_1}(w) & {\\partial g_2 \\over \\partial w_2}(w) & \\cdots & {\\partial g_2 \\over \\partial w_n}(w) \\\\\n",
    "\\cdots & \\cdots & \\cdots &\\cdots \\\\\n",
    "{\\partial g_n \\over \\partial w_1}(w) & {\\partial g_n \\over \\partial w_2}(w) & \\cdots & {\\partial g_n \\over \\partial w_n}(w)\n",
    "\\end{pmatrix}$(проверяется покоординатно)\n",
    "\n",
    "Посчитаем градиент:\n",
    "\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} w^TX^TXw - 2y^TXw +y^2= \\frac{\\partial}{\\partial w} (Xw)^TXw - \\frac{\\partial}{\\partial w} 2 y^TX w =  2X^TXw - 2 X^Ty$ (см. задачи 2 и 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Условие минимумма:\n",
    "\n",
    "$(2X^TXw - 2X^Ty) = 0$, \n",
    "\n",
    "значит:\n",
    "\n",
    "$\\hat{w} = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Замечание.**\n",
    "\n",
    "Найденная точка — минимум, если матрица $X^T X$ обратима. Из курса математического анализа известно, что если матрица Гессе функции положительно определёна в точке, градиент которой равен нулю, то эта точка является локальным минимумом.\n",
    "\n",
    "$\n",
    "\\nabla^2 Q(w) = 2X^TX.\n",
    "$\n",
    "\n",
    "Необходимо понять, является ли матрица $X^TX$ положительно определённой. Запишем определение положительной определённости матрицы  $X^TX$:\n",
    "\n",
    "$\n",
    "z^TX^TXz > 0, \\; \\forall z \\in \\mathbb{R}^d, z \\ne 0.\n",
    "$\n",
    "\n",
    "Видим, что тут записан квадрат нормы вектора $Xz$, то есть это выражение будет не меньше нуля. В случае, если матрица $X$ имеет «книжную» ориентацию (строк не меньше, чем столбцов) и имеет полный ранг (нет линейно зависимых столбцов), то вектор $Xz$ не может быть нулевым, а значит выполняется\n",
    "\n",
    "$\n",
    "z^TX^TXz = ||Xz||^2 > 0, \\; \\forall z \\in \\mathbb{R}^d, z \\ne 0.\n",
    "$\n",
    "\n",
    "То есть $X^TX$ является положительно определённой матрицей. Также, по критерию Сильвестра, все главные миноры (в том числе и определитель) положительно определённой матрицы положительны, а, следовательно, матрица $X^TX$ обратима, и решение существует. Если же строк оказывается меньше, чем столбцов, или $X$ не является полноранговой, то $X^TX$ необратима и решение $w$ определено неоднозначно. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
