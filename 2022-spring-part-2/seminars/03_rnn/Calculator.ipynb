{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m55TC4zOc9UC",
    "outputId": "8d5e66ea-73e2-46a3-be5a-7c0b52a0c87c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7RpaGTh6Sfw0"
   },
   "outputs": [],
   "source": [
    "all_letters = ['<START>', '<STOP>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '=']\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5e8EKW0ppDjS"
   },
   "outputs": [],
   "source": [
    "char_to_int = dict([(b,a) for a,b in enumerate(all_letters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8DrFc0Ngo-6a"
   },
   "outputs": [],
   "source": [
    "# One-hot matrix of first to last letters (START) for input\n",
    "def input_tensor(line):\n",
    "    tensor = torch.zeros(1, len(line) + 1, dtype=torch.int32)\n",
    "    tensor[0, 0] = char_to_int[\"<START>\"]\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[0, li+1] = char_to_int[letter]\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (STOP) for target\n",
    "def target_tensor(line):\n",
    "    letter_indexes = [char_to_int[line[li]] for li in range(0, len(line))]\n",
    "    letter_indexes.append(char_to_int[\"<STOP>\"]) \n",
    "    return torch.LongTensor([letter_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TsNgVaaXqYKn"
   },
   "outputs": [],
   "source": [
    "def random_training_sample():\n",
    "    a = random.randint(0, 999)\n",
    "    b = random.randint(0, 999)\n",
    "\n",
    "    in_str = str(a) + \"+\" + str(b) + \"=\" \n",
    "    out_str = str(a + b)\n",
    "\n",
    "    encode_line_tensor = input_tensor(in_str)\n",
    "    input_line_tensor = input_tensor(out_str)\n",
    "    target_line_tensor = target_tensor(out_str)\n",
    "    return encode_line_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  4,  4, 11, 12, 10,  4, 10, 13]], dtype=torch.int32),\n",
       " tensor([[0, 3, 2, 7, 9]], dtype=torch.int32),\n",
       " tensor([[3, 2, 7, 9, 1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_training_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "        \n",
    "        \n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 emb_dim = 10, \n",
    "                 hidden_dim = 10,\n",
    "                 num_layers = 3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # обучаемая матрица эмбедингов\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "        \n",
    "        self.encoder = torch.nn.GRU(\n",
    "            emb_dim, hidden_dim, num_layers)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros((1, self.num_layers, self.hidden_dim)).to(self.device)\n",
    "        \n",
    "    def forward(self, input, h):\n",
    "        input = self.embedding(input) # shape (batch_size, seq_len, emb_dim)\n",
    "        \n",
    "        h = torch.transpose(h, 0, 1)\n",
    "        \n",
    "        input = torch.transpose(input, 0, 1) # shape (seq_len, batch_size, emb_dim)\n",
    "        d, h = self.encoder(input, h)\n",
    "        return torch.transpose(h, 0, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 output_dim,\n",
    "                 emb_dim = 10, \n",
    "                 hidden_dim = 10,\n",
    "                 num_layers = 1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, self.emb_dim)\n",
    "\n",
    "        self.decoder = torch.nn.GRU(\n",
    "            emb_dim, hidden_dim, num_layers)\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros((1, self.num_layers, self.hidden_dim)).to(self.device)\n",
    "        \n",
    "    def forward(self, real, h):\n",
    "        batch_size = 1\n",
    "        \n",
    "        input = self.embedding(real)\n",
    "        input = torch.transpose(input, 0, 1)\n",
    "        h = torch.transpose(h, 0, 1)\n",
    "        d, _ = self.decoder(input, h)\n",
    "        answers = self.linear(d)\n",
    "            \n",
    "        return torch.transpose(answers, 0, 1)\n",
    "    \n",
    "    def generate(self, h, max_len=50):\n",
    "        input = self.embedding(\n",
    "                    torch.tensor([[char_to_int['<START>']]]).long().to(\n",
    "                        self.device\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "        input = torch.transpose(input, 0, 1)\n",
    "        h = torch.transpose(h, 0, 1)\n",
    "\n",
    "        answers = torch.zeros(\n",
    "                (max_len, input.shape[1], self.output_dim)).to(\n",
    "                    self.device)\n",
    "                \n",
    "        result = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            d, h = self.decoder(input, h)\n",
    "            answers[i, :, :] = self.linear(d)[0]\n",
    "            char = torch.argmax(answers[i:i+1, :, :], dim=-1)\n",
    "            result.append(char[0, 0].cpu().item())\n",
    "            input = self.embedding(char)\n",
    "            \n",
    "        s = \"\"\n",
    "        for r in result:\n",
    "            if r == char_to_int[\"<STOP>\"]:\n",
    "                break\n",
    "            s += all_letters[r]\n",
    "        return s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "T245sbctVowK"
   },
   "outputs": [],
   "source": [
    "def train_on_sample(optimizer, loss_function, model, random_training_sample):\n",
    "    rnn_encoder, rnn_decoder = model\n",
    "    \n",
    "    encode_line_tensor, input_line_tensor, target_line_tensor = random_training_sample\n",
    "    \n",
    "    #target_line_tensor.unsqueeze_(-1)\n",
    "\n",
    "    hidden = rnn_encoder.init_hidden()\n",
    "\n",
    "    hidden = rnn_encoder(encode_line_tensor, hidden)\n",
    "\n",
    "    output = rnn_decoder(input_line_tensor, hidden)\n",
    "    \n",
    "    output = torch.transpose(output, 1, 2)\n",
    "    \n",
    "    loss = loss_function(output, target_line_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Okd9JRTdhNf",
    "outputId": "dc16a936-10a1-4b3a-e699-40a429db5241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50 1%) 2.4555\n",
      "(100 2%) 2.0458\n",
      "(150 3%) 1.9165\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qm/772_n5j93cq0r6x7c4qmrt6m0000gn/T/ipykernel_95114/1212962909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_training_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qm/772_n5j93cq0r6x7c4qmrt6m0000gn/T/ipykernel_95114/4259678262.py\u001b[0m in \u001b[0;36mtrain_on_sample\u001b[0;34m(optimizer, loss_function, model, random_training_sample)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-course/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-course/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-course/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-course/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_encoder = Encoder(n_letters, 256, 256, \n",
    "                      num_layers = 3)\n",
    "\n",
    "rnn_decoder = Decoder(vocab_dim = n_letters,\n",
    "                      output_dim = n_letters,\n",
    "                      emb_dim = 256, \n",
    "                      hidden_dim = 256,\n",
    "                      num_layers = 3)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(rnn_encoder.parameters(),\n",
    "                                             rnn_decoder.parameters()), \n",
    "                             lr=learning_rate)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "n_iters = 5000\n",
    "print_every = 50\n",
    "total_loss = 0 \n",
    "\n",
    "model = (rnn_encoder, rnn_decoder)\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train_on_sample(optimizer, loss_function, model, random_training_sample())\n",
    "    total_loss += loss.cpu().item()\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        print('(%d %d%%) %.4f' % (iter, iter / n_iters * 100, total_loss / print_every))\n",
    "        total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cRiTf1bhhpM"
   },
   "outputs": [],
   "source": [
    "max_length=30\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(word):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        encode_line_tensor = input_tensor(word)\n",
    "\n",
    "        hidden = rnn_encoder.init_hidden()\n",
    "\n",
    "        hidden = rnn_encoder(encode_line_tensor, hidden)\n",
    "\n",
    "        output = rnn_decoder.generate(hidden)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "K2j-6760h4jx",
    "outputId": "5f39a5d9-051d-4dd1-cb0e-93ff5c032c35"
   },
   "outputs": [],
   "source": [
    "sample('2+2=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "MzqGopsnrhrX",
    "outputId": "0670aca4-641d-4119-c355-2e0832de90a3"
   },
   "outputs": [],
   "source": [
    "sample('144+543=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV9uzlm7hpE_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Calculator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
