{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/logo_fmkn.png\" alt=\"logo_fmkn\" />\n",
    "</div>\n",
    "\n",
    "# Машинное обучение 1\n",
    "\n",
    "### Лекция 14. Многомерная линейная регрессия. Метод главных компонент\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "17 декабря 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пятиминутка\n",
    "\n",
    "1. Выпишите формулу расстояния Кульбака-Лейблера\n",
    "2. В чем основная идея ЕМ-алгоритма, какая задача решается на Е-шаге, на М-шаге?\n",
    "3. Расшифруйте аббревиатуру PPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/LR_mem.jpg\" alt=\"LR_mem\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Линейная регрессия (напоминание)\n",
    "\n",
    "$X$ — объекты (часто $\\mathbb{R}^n$); $Y$ – ответы (часто $\\mathbb{R}$, реже $\\mathbb{R}^m$)\n",
    "\n",
    "$X^\\ell = (x_i, y_i)^\\ell_{i=1}$ — обучающая выборка;\n",
    "\n",
    "$y_i = y(x_i), y: X \\to Y$ – неизвестная зависимость;\n",
    "\n",
    " * $a(x) = f(x, \\alpha)$ — модель зависимости, $\\alpha \\in \\mathbb{R}^p$ — вектор параметров модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод наименьших квадратов (MHK, напоминание)\n",
    " \n",
    " $Q(\\alpha, X^\\ell) = \\sum\\limits_{i=1}^\\ell w_i(f(x_i, \\alpha) - y_i)^2 \\to \\min\\limits_{\\alpha}$\n",
    "\n",
    "где $w_i$ — вес, степень важности $i$-го объекта.\n",
    "\n",
    "$Q(\\alpha, X^\\ell)$ — остаточная сумма квадратов (residual sum of squares, RSS).\n",
    "\n",
    "Можно увидеть, что постановки МНК и метода максимального правдоподобия совпадают, если положить веса объектов обратно пропорциональными дисперсии шума: $w_i = \\sigma_i^{-2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Многомерная линейная регрессия\n",
    "\n",
    "$f_1(x), \\dots, f_n(x)$ — числовые признаки\n",
    "\n",
    "Модель многомерной линейной регрессии:\n",
    "\n",
    "$f(x, \\alpha) = \\sum\\limits_{j=1}^n \\alpha_j f_j (x), \\alpha \\in \\mathbb{R}^n$\n",
    "\n",
    "Матричные обозначения:\n",
    "\n",
    "$F_{\\ell \\times n} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   f_1(x_1) & \\dots & f_n(x_1) \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   f_1(x_\\ell) & \\dots & f_n(x_\\ell)\n",
    "  \\end{array} \\right],\n",
    "y_{\\ell \\times 1} = \\left[ \n",
    "\\begin{array}{c}\n",
    "   y_1 \\\\\n",
    "     \\dots \\\\\n",
    "   y_\\ell\n",
    "  \\end{array} \n",
    "\\right],\n",
    "\\alpha_{n\\times 1} = \\left[ \n",
    "\\begin{array}{c}\n",
    "   \\alpha_1 \\\\\n",
    "     \\dots \\\\\n",
    "   \\alpha_n\n",
    "  \\end{array} \n",
    "\\right]\n",
    "$\n",
    "\n",
    "Функционал квадрата ошибки:\n",
    "\n",
    " $Q(\\alpha, X^\\ell) = \\sum\\limits_{i=1}^\\ell (f(x_i, \\alpha) - y_i)^2 = \\|F\\alpha - y \\|^2 \\to \\min\\limits_{\\alpha}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Нормальная система уравнений\n",
    "\n",
    "Необходимое условие минимума в матричном виде:\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial \\alpha} (\\alpha) = 2F^T(F\\alpha - y) = 0$,\n",
    "\n",
    "откуда следует нормальная система задачи МНК:\n",
    "\n",
    "$ F^TF\\alpha = F^T y$,\n",
    "\n",
    "где $F^TF$ — матрица размера $n \\times n$, ковариационная матрица набора признаков\n",
    "\n",
    "**Решение системы**: $\\alpha^* = (F^TF)^{-1}F^Ty = F^+ y $.\n",
    "\n",
    "Значение функционала: $Q(\\alpha^*) =\\|P_Fy - y \\|^2$,\n",
    "\n",
    "где $P_F = FF^+ = F(F^TF)^{-1}F^T $ — проекционная матрица\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Геометрическая интерпретация МНК\n",
    "\n",
    "Линейная оболочка столбцов матрицы $F = (f_1, \\dots, f_n), f_j \\in \\mathbb{R}^\\ell$:\n",
    "\n",
    "$\\mathscr{L}(F) = \\sum\\limits_{j=1}^n \\alpha_j f_j , \\ \\alpha \\in \\mathbb{R}^n$\n",
    "\n",
    "$P_F = F(F^TF)^{-1}F^T$ — проекционная матрица\n",
    "\n",
    "$P_Fy$ – проекция вектора $y \\in \\mathbb{R}^\\ell$ на подпространство $\\mathscr{L}(F)$\n",
    "\n",
    "$(I_\\ell - P_F)y$ — проекция $y$ на его ортогональное дополнение\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/P_F_y.jpg\" alt=\"P_F_y\" width=600 />\n",
    "</div>\n",
    "\n",
    "МНК – это опускание перпендикуляра в $\\mathbb{R}^\\ell$ из $y$ на $\\mathscr{L}(F)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Сингулярное разложение\n",
    "\n",
    "Произвольная $\\ell\\times n$-матрица представима в виде _сингулярного разложения_ (singular value decomposition, SVD):\n",
    "\n",
    "$F = VDU^T$\n",
    "\n",
    "**Основные свойства** сингулярного разложения:\n",
    "1. $\\ell\\times n$-матрица $V = (v_1, \\dots, v_n)$ ортогональна, $V^TV = I_n$, столбцы $v_j$ — собственные векторы матрицы $FF^T$\n",
    "2. $n\\times n$-матрица $U = (u_1, \\dots, u_n)$ ортогональна, $U^TU = I_n$, столбцы $u_j$ — собственные векторы матрицы $F^TF$\n",
    "3. $n\\times n$-матрица $D$ диагональна, $D = \\text{diаg}(\\sqrt{\\lambda_1}, \\dots, \\sqrt{\\lambda_n}), \\lambda_j \\geq 0$ — собственные значения матриц $F^TF$ и $FF^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Решение МНК через сингулярное разложение\n",
    "\n",
    "Псевдообратная $F^+$, вектор MHK-решения $\\alpha^*$, МНК-аппроксимация целевого вектора $F\\alpha^*$:\n",
    "\n",
    "$F^+ = (UDV^TVDU^T)^{-1} UDV^T = UD^{-1}V^T = \\sum\\limits_{j=1}^n \\color{red}{\\frac{1}{\\sqrt{\\lambda_j}}} u_j v_j^T$\n",
    "\n",
    "$\\alpha^* = F^+ y = UD^{-1}V^T y = \\sum\\limits_{j=1}^n \\color{red}{\\frac{1}{\\sqrt{\\lambda_j}}} u_j (v_j^T y)$\n",
    "\n",
    "$F\\alpha^* = P_F y = (VDU^T)UD^{-1}V^Ty = VV^T y = \\sum\\limits_{j=1}^n v_j (v_j^T y)$\n",
    "\n",
    "$\\|\\alpha^*\\|^2 = \\|U D^{-1} V^T y \\|^2 = \\|D^{-1} V^T y \\|^2 = \\sum\\limits_{j=1}^n \\color{red}{\\frac{1}{\\sqrt{\\lambda_j}}} (v_j^T y)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Проблема мультиколлинеарности\n",
    "\n",
    "Если $\\exists \\gamma \\in \\mathbb{R}^n : F \\gamma \\approx 0$, то некоторые $\\lambda_j$ близки к нулю. \n",
    "\n",
    "Определим *число обусловленности* матрицы $\\underset{n \\times n}{S}$:\n",
    "\n",
    "$\\mu(S) = \\|S \\| \\|S^{-1} \\| = \\frac{\\max\\limits_{u: \\|u\\| = 1} \\| Su\\|}{\\min\\limits_{u: \\|u\\| = 1} \\| Su\\|} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$\n",
    "\n",
    "При умножении обратной матрицы на вектор, $z = S^{-1}u$, относительная погрешность усиливается в $\\mu(S)$ раз:\n",
    "\n",
    "$\\frac{\\|\\delta z\\|}{\\|z\\|} \\leq \\mu(S) \\frac{\\|\\delta u\\|}{\\|u\\|} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Проблема мультиколлинеарности и переобучения\n",
    "\n",
    "Если матрица $S = F^TF$ плохо обусловлена, то:\n",
    " * решение неустойчиво и плохо интерпретируемо\n",
    " * $\\|\\alpha^* \\|$ велика\n",
    " * возникает переобучение:\n",
    "   - на обучении $Q(\\alpha^*, X^\\ell) = \\| F\\alpha^* - y\\|^2$ мало\n",
    "   - на контроле $Q(\\alpha^*, X^k) = \\| F^\\prime\\alpha^* - y^\\prime\\|^2$ велико\n",
    "\n",
    "Стратегии устранения мультиколлинеарности и переобучения:\n",
    " * регуляризация: $\\| \\alpha \\| \\to \\min$\n",
    " * отбор признаков:$f_1, \\dots, f_n \\to f_{j_1}, \\dots, f_{j_m}, m \\ll n$\n",
    " * преобразование признаков: $f_1, \\dots, f_n \\to g_1, \\dots, g_m, m \\ll n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 1:</b> Почему $F^\\prime$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Регуляризация (гребневая регрессия)\n",
    "\n",
    "Штраф за увеличение нормы вектора весов $\\|\\alpha\\|$:\n",
    "\n",
    "$Q_{\\tau} (\\alpha) = \\|F \\alpha - y \\|^2 + \\frac{1}{\\sigma} \\| \\alpha\\|^2$\n",
    "\n",
    "где $\\tau = \\frac{1}{\\sigma}$ – неотрицательный параметр регуляризации.\n",
    "\n",
    "**Вероятностная интерпретация**: априорное распределение вектора $\\alpha$ — гауссовское с ковариационной матрицей $\\sigma I_n$\n",
    "\n",
    "Модифицированное МНК-решение:\n",
    "\n",
    "$\\alpha_\\tau^* = (F^TF + \\tau I_n)^{-1} F^T y$\n",
    "\n",
    "Преимущество сингулярного разложения: можно подбирать параметр $\\tau$, вычислив SVD только один раз.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Регуляризованный МНК через сингулярное разложение\n",
    "\n",
    "Вектор регуляризованного МНК-решения $\\alpha_\\tau^*$ и MHK-аппроксимация целевого вектора $F\\alpha_\\tau^*$:\n",
    "\n",
    "$\\alpha_\\tau^* = U(D^2 + \\tau I_n)^{-1} D V^T y = \\sum\\limits_{j=1}^n \\color{red}{\\frac{\\sqrt{\\lambda_j}}{\\lambda_j + \\tau}} u_j (v_j^T y)$\n",
    "\n",
    "$F\\alpha_\\tau^* = VDU^T \\alpha_\\tau^* = V\\text{diag} \\left(\\frac{\\lambda_j}{\\lambda_j + \\tau} \\right) = \\sum\\limits_{j=1}^n \\color{red}{\\frac{\\lambda_j}{\\lambda_j + \\tau}} v_j (v_j^T y)$\n",
    "\n",
    "$\\|\\alpha_\\tau^*\\|^2 = \\| (D^2 + \\tau I_n)^{-1} D V^T y \\|^2 = \\sum\\limits_{j=1}^n \\color{red}{\\frac{\\lambda_j}{(\\lambda_j + \\tau)^2}} (v_j^T y)^2$\n",
    "\n",
    "$F\\alpha_\\tau^* \\neq F\\alpha^*$, но зато решение становится гораздо устойчивее.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Выбор параметра регуляризации $\\tau$\n",
    "\n",
    "Контрольная выборка: $X^k = (x_i^\\prime, y_i^\\prime)^k_{i=1}$\n",
    "\n",
    "$F^\\prime_{k \\times n} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   f_1(x^\\prime_1) & \\dots & f_n(x^\\prime_1) \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   f_1(x^\\prime_k) & \\dots & f_n(x^\\prime_k)\n",
    "  \\end{array} \\right],\n",
    "y^\\prime_{k\\times 1} = \\left[ \n",
    "\\begin{array}{c}\n",
    "   y^\\prime_1 \\\\\n",
    "     \\dots \\\\\n",
    "   y^\\prime_k\n",
    "  \\end{array} \n",
    "\\right]\n",
    "$\n",
    "\n",
    "Вычисление функционала $Q$ на контрольных данных $T$ раз потребует $O(kn^2 + knT)$ операций:\n",
    "\n",
    "$Q(\\alpha_\\tau^*, X^k) = \\|F^\\prime \\alpha_\\tau^* - y^\\prime \\|^2 = \\|\\underbrace{F^\\prime U}_{k\\times n} \\text{diag} \\left(\\frac{\\sqrt{\\lambda_j}}{\\lambda_j + \\tau}\\right) \\underbrace{V^Ty}_{n \\times 1} - y^\\prime \\|^2$\n",
    "\n",
    "Зависимость $Q(\\tau)$ обычно имеет характерный минимум.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Регуляризация сокращает «эффективную размерность»\n",
    "\n",
    "Сжатие (shrinkage) или сокращение весов (weight decay):\n",
    "\n",
    "$\\|\\alpha_\\tau^* \\|^2 = \\sum\\limits_{j=1}^n \\color{red}{\\frac{\\lambda_j}{(\\lambda_j + \\tau)^2}} (v_j^T y)^2 < \\|\\alpha^* \\|^2 = \\sum\\limits_{j=1}^n \\color{red}{\\frac{1}{\\lambda_j}} (v_j^T y)^2$\n",
    "\n",
    "Говорят, что имеет место сокращение эффективной размерности\n",
    "\n",
    "Роль размерности играет след проекционной матрицы:\n",
    "\n",
    "$\\text{tr} F(F^TF)^{-1}F^T = \\text{tr}(F^TF)^{-1}F^TF = \\text{tr} I_n = n$\n",
    "\n",
    "При использовании регуляризации:\n",
    "\n",
    "$\\text{tr} F(F^TF + \\tau I_n)^{-1}F^T = \\text{tr diag}\\left( \\frac{\\lambda_j}{\\lambda_j + \\tau} \\right) = \\sum\\limits_{j=1}^n \\frac{\\lambda_j}{\\lambda_j + \\tau} < n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 2:</b> Почему внутри следа tr можно циклически переставлять матрицы?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/pca_mem.jpg\" alt=\"pca_mem\" width=600 />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод главных компонент: постановка задачи\n",
    "\n",
    "$f_1(x), \\dots, f_n(x)$ — исходные числовые признаки\n",
    "\n",
    "$g_1(x), \\dots, g_m(x)$ — новые числовые признаки, $m \\leq n$\n",
    "\n",
    "**Требование**: старые признаки должны линейно восстанавливаться по новым как можно точнее на обучающей выборке $X^\\ell$\n",
    "\n",
    "$ \\hat f_j(x) = \\sum\\limits_{s=1}^m g_s(x) u_{js}, j=1,\\dots,n, \\forall x \\in X$\n",
    "\n",
    "$ \\sum\\limits_{i=1}^\\ell \\sum\\limits_{j=1}^n (\\hat f_j(x) - f_j(x))^2 \\to \\min\\limits_{\\{g_s(x_i)\\}, \\{u_{js}\\}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Матричные обозначения\n",
    "\n",
    "Матрицы «объекты-признаки», старая и новая:\n",
    "\n",
    "$F_{\\ell \\times n} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   f_1(x_1) & \\dots & f_n(x_1) \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   f_1(x_\\ell) & \\dots & f_n(x_\\ell)\n",
    "  \\end{array} \\right]\n",
    "\\ \\ \\ \n",
    "G_{\\ell \\times m} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   g_1(x_1) & \\dots & g_m(x_1) \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   g_1(x_\\ell) & \\dots & g_m(x_\\ell)\n",
    "  \\end{array} \\right]\n",
    "$\n",
    "\n",
    "Матрица линейного преобразования новых признаков в старые:\n",
    "\n",
    "$U_{n \\times m} =  \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   u_{11} & \\dots & u_{1m} \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   u_{n1} & \\dots & u_{nm}\n",
    "  \\end{array} \\right],\n",
    "  \\hat F = GU^T \\underset{\\text{хотим}}{\\approx} F\n",
    "$\n",
    "\n",
    "**Найти**: и новые признаки $G$, и преобразование $U$:\n",
    "\n",
    "$\\sum\\limits_{i=1}^\\ell \\sum\\limits_{j=1}^n (\\hat f_j(x_i) - f_j(x_i))^2 = \\| GU^T - F\\|^2 \\to \\min\\limits_{G,U}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Основная теорема метода главных компонент\n",
    "\n",
    "**Теорема**\n",
    "\n",
    "Если $m \\leq \\text{rk} F$, то минимум $\\|GU^T - F\\|^2$ достигается, когда столбцы $U$ — это с.в. матрицы $F^TF$, \n",
    "соответствующие $m$ максимальным с.з. $\\lambda_1, \\dots, \\lambda_m$, а матрица $G = FU$.\n",
    "\n",
    "При этом:\n",
    " * матрица $U$ ортонормирована: $U^TU = I_m$\n",
    " * матрица $G$ ортогональна $G^TG = \\Lambda = \\text{diag} (\\lambda_1, \\dots, \\lambda_m)$\n",
    " * $U \\Lambda = F^TFU, G\\Lambda = FF^TU$\n",
    " * $\\|GU^T - F \\|^2 = \\| F\\|^2 - \\text{tr} \\Lambda = \\sum\\limits_{j=m+1}^n \\lambda_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Связь с сингулярным разложением\n",
    "\n",
    "Если взять $m = n$, то:\n",
    " * $\\|GU^T - F \\|^2 = 0$\n",
    " * представление $\\hat F = GU^T = F$ точное и совпадает с сингулярным разложением при $G = V \\sqrt{\\Lambda}$:\n",
    "\n",
    " $F = GU^T = V \\sqrt{\\Lambda} U^T,\\ U^TU = I_m,\\ V^TV = I_m$\n",
    "\n",
    " * линейное преобразование $U$ работает в обе стороны:\n",
    "\n",
    " $F = GU^T,\\ G = FU$\n",
    "\n",
    "Поскольку новые признаки некоррелированы ($G^TG = \\Lambda$), преобразование $U$ называется декоррелирующим (или преобразованием Карунена-Лоэва)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/multicollinearity_meme.jpg\" alt=\"multicollinearity_meme\" width=800 />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Эффективная размерность выборки\n",
    "\n",
    "Упорядочим с.з. $F^TF$ по убыванию: $\\lambda_1 \\geq \\dots \\geq \\lambda_n \\geq 0$.\n",
    "\n",
    "Эффективная размерность выборки — это наименьшее целое $m$, при котором\n",
    "\n",
    "$E_m = \\frac{\\|GU^T - F\\|^2}{\\|F\\|^2} = \\frac{\\lambda_{m+1} + \\dots + \\lambda_n}{\\lambda_1 + \\dots + \\lambda_n} \\leq \\varepsilon$\n",
    "\n",
    "Критерий «крутого склона»: находим $m$ такое, что $E_{m-1} \\gg E_{m}$:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/steep_slope.jpg\" alt=\"steep_slope\" width=800/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Решение задачи наименьших квадратов в новых признаках\n",
    "\n",
    "Заменим $\\underset{\\ell\\times n}{F}$ на её приближение $\\underset{\\ell\\times m}{G} \\cdot \\underset{m\\times n}{U^T}$, предполагая $m \\leq n$:\n",
    "\n",
    "$\\|G\\underbrace{U^T\\alpha}_{\\beta} - y\\|^2 = \\|G\\beta - y\\|^2 \\to \\min\\limits_{\\beta}$\n",
    "\n",
    "Связь нового и старого вектора коэффициентов:\n",
    "\n",
    "$\\beta = U^T \\alpha\\ \\ \\ \\alpha=U\\beta$\n",
    "\n",
    "Решение задачи наименьших квадратов относительно $\\beta$ (единственное отличие — $m$ слагаемых вместо $n$):\n",
    "\n",
    "$\\beta^* = D^{-1}V^Ty\\ \\ \\alpha^* = UD^{-1}V^T y = \\sum\\limits_{j=1}^\\color{red}{m} \\frac{1}{\\sqrt{\\lambda_j}}u_j (v_j^T y)$\n",
    "\n",
    "$G\\beta^* = VV^T y = \\sum\\limits_{j=1}^\\color{red}{m} v_j (v_j^T y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обобщение. Спектральный метод наименьших квадратов\n",
    "\n",
    "1. Построить SVD-разложение: $\\lambda_1 \\geq \\dots \\geq \\lambda_m \\geq \\color{red}{\\lambda_{m+1} \\geq \\dots \\geq \\lambda_{n}}$\n",
    "2. Игнорировать $n-m$ наименьших с. з. или иным способом отделить c. з. от нуля: $\\lambda_j^\\prime = f(\\lambda_j)$\n",
    "\n",
    "Частные случаи:\n",
    " * $\\lambda_j^\\prime = \\lambda_j + \\tau$ — гребневая регрессия\n",
    " * $\\lambda_j^\\prime = \\lambda_j[j \\leq m]$ — метод главных компонент\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Применить формулы SVD для модификации МНК-решения:\n",
    "\n",
    " $\\alpha^* = \\sum\\limits_{j=1}^n \\color{red}{\\frac{1}{\\sqrt{\\lambda_j}}} u_j (v_j^T y) \\to \n",
    "  \\alpha^* = \\sum\\limits_{j=1}^\\color{red}{m} \\color{red}{\\frac{\\sqrt{\\lambda_j}}{\\lambda_j^\\prime}} u_j (v_j^T y)\n",
    " $\n",
    " \n",
    " $ F \\alpha^* = \\sum\\limits_{j=1}^n v_j(v_j^Ty) \\to F \\alpha^* = \\sum\\limits_{j=1}^n v_j(v_j^Ty) =  \n",
    "   \\sum\\limits_{j=1}^\\color{red}{m} \\color{red}{\\frac{\\lambda_j}{\\lambda_j^\\prime}} v_j (v_j^T y)\n",
    " $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обобщение. Задачи низкорангового матричного разложения\n",
    "\n",
    " * Понижение размерности в задачах регрессии\n",
    " * Понижение размерности в задачах классификации\n",
    " * Формирование сжатого представления данных\n",
    " \n",
    "**Дано**: матрица $Z = \\| z_{ij}\\|_{n\\times m}, (i,j) \\in \\Omega \\subset \\{1..n\\} \\times \\{1..m\\} $\n",
    "\n",
    "**Найти**: $X = \\|x_{it}\\|_{n\\times k}$ и $Y = \\|y_{tj}\\|_{k\\times m}$ такие, что\n",
    "\n",
    " $\\|Z - XY \\|^2 = \\sum\\limits_{(i,j) \\in \\Omega} \\left(z_{ij} - \\sum\\limits_t x_{it}y_{tj} \\right)^2 \\to \\min_{X, Y}$\n",
    "\n",
    "Дополнительные ограничения, вынуждающие отказаться от SVD:\n",
    " * неквадратичная функция потерь\n",
    " * неотрицательное матричное разложение: $x_{it} \\geq 0,\\ y_{tj} \\geq 0$\n",
    " * разреженные данные: $|\\Omega| \\ll nm$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 3:</b> Почему указанные ограничения могут быть недостатками?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Примеры прикладных задач матричного разложения\n",
    "\n",
    "1. Выявление интересов в рекомендательных системах (recommender systems, collaborative filtering)\n",
    "\n",
    " $z_{iu} = \\sum\\limits_t p_{it}q_{tu}$\n",
    "\n",
    "**дано**: $z_{iu}$ — рейтинги товаров $i$, поставленные пользователем $u$\n",
    "\n",
    "**найти**: $p_{it}$ — профиль интересов товара $i$\n",
    "\n",
    "$q_{tu}$ — профиль интересов пользователя $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Разделение смеси химических веществ по данным жидкостной хроматографии\n",
    "\n",
    " $z_{t\\lambda} = \\sum\\limits_i x_{ti}y_{i\\lambda}$\n",
    "\n",
    "**дано**: $z_{t\\lambda}$ — выход сканирующего УФ-детектора\n",
    "\n",
    "**найти**: $x_{ti}$ — хроматограмма $i$-го вещества, $t$ — время\n",
    "\n",
    "   $y_{i\\lambda}$ — спектр $i$-го вещества, $\\lambda$ — длина волны\n",
    "\n",
    "3. Латентный семантический анализ коллекций текстов (тематическое моделирование)\n",
    "\n",
    " $z_{wd} = \\sum\\limits_t \\phi_{wt} \\theta_{td}$\n",
    "\n",
    "**дано**: $z_{wd} = p(w|d)$ — частоты слов $w$ в документах $d$\n",
    "\n",
    "**найти**: $\\phi_{wt} = p(w|t)$ — распределения слов $w$ в темах $t$\n",
    "\n",
    " $\\theta_{td} = p(t|d)$ — распределения тем $t$ в документах $d$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Резюме\n",
    "\n",
    " * Многомерная линейная регрессия\n",
    "   - через сингулярное разложение\n",
    " * Гребневая регрессия\n",
    "   - тоже через сингулярное разложение\n",
    " * Метод главных компонент\n",
    "   - тоже через сингулярное разложение\n",
    " * Матричные разложения\n",
    "   - SVD и др. (NNMF, ALS, PLSA — во второй части курса)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Спасибо за внимание и до встречи в следующем семестре!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
