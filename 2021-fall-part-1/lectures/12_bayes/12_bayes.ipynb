{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/logo_fmkn.png\" alt=\"logo_fmkn\" />\n",
    "</div>\n",
    "\n",
    "# Машинное обучение\n",
    "\n",
    "### Лекция 13. Введение в байесовские методы\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "3 декабря 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пятиминутка\n",
    "\n",
    "1. Является ли задача кластеризации корректной? Почему?\n",
    "2. Опишите постановку задачи semi-supervised learning\n",
    "3. В чем идея алгоритма кластеризации DBSCAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Формула Байеса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Условная вероятность\n",
    "\n",
    "$p(y|x) =\\frac{p(x, y)}{p(x)}$\n",
    "\n",
    "**Пример**\n",
    "\n",
    "Вероятность впадения 6 на игральной кости при условии, что выпало чётное число.\n",
    "\n",
    "$$p(6|even) = \\frac{p(even, 6)}{p(even)} = \\frac13$$\n",
    "\n",
    "Симметричность: $$p(y|x)p(x) = p(x, y) = p(x|y)p(y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Маргинализация\n",
    "\n",
    "$$p(y|x)p(x) = p(x, y) = p(x|y)p(y) \\Rightarrow$$\n",
    "\n",
    "$$\\Rightarrow p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "$$\\overset{\\int dy}{\\Rightarrow}  p(x) = \\int p(x|y)p(y)dy = \\int p(x, y)dy = E_y p(x|y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Правила суммы и произведения\n",
    "\n",
    "**Правило суммы:**\n",
    "\n",
    "$$p(x_1, \\dots , x_k) = \\int p(x_1, \\dots , x_k, \\dots , x_n)d_{x_{k+1}} \\dots d_{x_n}$$\n",
    "\n",
    "**Правило произведения:**\n",
    "\n",
    "$$p(x_1, x_2, x_3, \\dots , x_{n−1}, x_n) = p(x_1|x_2, x_3, \\dots, x_{n−1}, x_n)\\cdot \\\\ \\cdot p(x_2|x_3, \\dots, x_{n−1}, x_n) \\dots p(x_{n−1})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Формула Байеса\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "p(y|x) = \\frac{p(x|y)p(y)}{p(x)} \\\\\n",
    "p(x) = \\int p(x|y)p(y)dy\n",
    "\\end{cases}\n",
    "\\Rightarrow\n",
    "$\n",
    "\n",
    "$p(y|x) = \\frac{p(x|y)p(y)}{\\int p(x|y)p(y)dy}$\n",
    "\n",
    "$Posterior = \\frac{Likelihood\\ \\cdot Prior}{Evidence}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пример с другом и экзаменом\n",
    "\n",
    "**Дано:**\n",
    "\n",
    "x — студент (не)рад\n",
    "\n",
    "y — экзамен (не)сдан\n",
    "\n",
    "$p(\\text{экзамен сдан}) = \\frac13$\n",
    "\n",
    "$p(\\text{студент рад}|\\text{экзамен сдан}) = 1$\n",
    "\n",
    "$p(\\text{студент рад}|\\text{экзамен НЕ сдан}) = \\frac16$\n",
    "\n",
    "**Найти:**\n",
    "\n",
    "$p(y=1|x=1) = p(\\text{экзамен сдан}|\\text{студент рад})\\ — ?$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Решение:**\n",
    "\n",
    "$p(\\text{экзамен сдан}|\\text{студент рад}) = \\frac{p(\\text{студент рад}|\\text{экзамен сдан})p(\\text{экзамен сдан})}{p(\\text{студент рад})} = $\n",
    "\n",
    "$ = \\frac{p(\\text{студент рад}|\\text{экзамен сдан})p(\\text{экзамен сдан})}{p(\\text{студент рад},\\text{экзамен сдан}) + p(\\text{студент рад},\\text{экзамен НЕ сдан})} = $\n",
    "\n",
    "$ = \\frac{p(\\text{студент рад}|\\text{экзамен сдан})p(\\text{экзамен сдан})}{p(\\text{студент рад}|\\text{экзамен сдан})p(\\text{экзамен сдан}) + p(\\text{студент рад}|\\text{экзамен НЕ сдан})p(\\text{экзамен НЕ сдан})} = \\\\ \\frac{\\frac{1}{3}}{\\frac{1}{3} + \\frac{1}{6}\\cdot\\frac{2}{3}} = \\frac{3}{4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Графические модели\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/graph.jpg\" alt=\"graph\" width=200 />\n",
    "</div>\n",
    "\n",
    "$$p(x_1, x_2, x_3, x_4, x_5) = p(x_1 | x_3) p(x_2 | x_4)p(x_3 | x_4, x_5)p(x_4)p(x_5)$$\n",
    "\n",
    "Например, что влияет на настроение друга?\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/glad_student.jpg\" alt=\"glad_student\" width=200 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод максимума правдоподобия (частотный подход)\n",
    "\n",
    "$X = {x_1, x_2, \\dots, x_n}, x_i ∼ p(x|\\theta)$\n",
    "\n",
    "$\\theta_{\\text{ML}} = \\arg\\max\\limits_\\theta p(X|θ) = \\arg\\max\\limits_\\theta \\prod\\limits_{i=1}^n p(x_i|\\theta)\n",
    "= \\arg\\max\\limits_\\theta \\sum\\limits_{i=1}^n \\log p(x_i|\\theta)$\n",
    "\n",
    "Свойства:\n",
    " * оценка ММП состоятельна (consistency): $\\theta_{\\text{ML}} \\overset{p}{\\to} \\theta_{\\text{ground truth}}$\n",
    " * асимптотически эффективна: не существует состоятельной оценки, улучшающей среднеквадратичную ошибку при $n \\to \\infty$\n",
    " * асимптотически нормальна: $\\sqrt{n} \\left(\\theta_{\\text{ML}} - \\theta_{\\text{ground truth}}\\right) \\overset{d}{\\to} N(0, I^{-1})$\n",
    " \n",
    " $I = -\\lim\\limits_{n \\to \\infty} \\frac1n \\mathbb{E}(\\text{Hessian})$ — асимптотическая информационная матрица (Фишера)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Байесовский подход к задаче машинного обучения\n",
    "\n",
    "$$p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{\\int p(X|\\theta)p(\\theta)d\\theta}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 1:</b> Какие недостатки и преимущества у частотного и байесовского подходов?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Сравнение подходов\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <th> </th>\n",
    "    <th>Частотный подход (Фишер)</th>\n",
    "    <th>Байесовский подход</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Отношение к случайности</td>\n",
    "    <td>никак не предсказать</td>\n",
    "    <td>можно предсказать при наличии достаточного количества информации</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Величины</td>\n",
    "    <td>случайные и детерминированные</td>\n",
    "    <td>все случайные</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Метод вывода</td>\n",
    "    <td>максимальное правдоподобие</td>\n",
    "    <td>Теорема Байеса</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Оценки параметров</td>\n",
    "    <td>точечные</td>\n",
    "    <td>апостериорное распределение</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Случаи применимости</td>\n",
    "    <td>$n \\gg d$</td>\n",
    "    <td>всегда</td>\n",
    "  </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Связь частотного и байесовского подходов\n",
    "\n",
    "$$\\lim\\limits_{n \\to \\infty} p(\\theta|x_1, \\dots, x_n) = \\delta(\\theta - \\theta_{\\text{ML}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Достоинства байесовского подхода\n",
    "\n",
    " * Регуляризация\n",
    " * Композитность. Например, популярность тем в соц.сетях — сначала по ленте новостей посчитали, потом уточнили по видео\n",
    " * Обработка данных «на лету» — итеративно обновляем апостериорное распределение\n",
    " * Можем считать различные статистики\n",
    " * Можем оценивать уверенность в значениях параметров модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Оценка уверенности\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/robustness.jpg\" alt=\"robustness\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вычислительные трудности\n",
    "\n",
    "$$p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{\\underbrace{\\int p(X|\\theta)p(\\theta)d\\theta}_{\\text{многомерный интеграл}}}$$\n",
    "\n",
    "\n",
    "#### Методы вычисления\n",
    " * Аналитическое вычисление\n",
    " * Markov chain Monte Carlo (MCMC)\n",
    " * Вариационный вывод (не рассматриваем в этом курсе)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Аналитический подход (сопряжённые распределения)\n",
    "\n",
    "Пусть $p(x|\\theta) = A(\\theta)$ и $p(\\theta) = B(\\alpha)$. \n",
    "\n",
    "Априорное распределение параметров $p(\\theta)$ называется сопряжённым к $p(x|\\theta)$ (англ. — conjugate priors), если апостериорное распределение принадлежит тому же семейству, что и априорное $$p(\\theta|x) = B(\\alpha^\\prime)$$\n",
    "\n",
    "Зная формулу семейства, которому должно принадлежать наше апостериорное распределение, легко вычислить его нормирующий множитель (интеграл в знаменателе).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пример 1: вычисление апостериорного распределения матожидания нормального распределения\n",
    "\n",
    "Нормальное распределение\n",
    "\n",
    "$N(x|\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)$\n",
    "\n",
    "\n",
    "Правдоподобие:\n",
    "\n",
    "$p(x|\\mu) = N(x|\\mu, 1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2} \\right)$\n",
    "\n",
    "Сопряжённое априорное:\n",
    "\n",
    "$p(\\mu) = N(\\mu|m,s^2)$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/gauss_graph.jpg\" alt=\"gauss_graph\" width=400 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2: апостериорная вероятность выпадения орла при подкидывании нечестной монеты\n",
    "\n",
    "**Частотный подход**\n",
    "\n",
    "$q$ — вероятность орла равна доле выпавших орлов при $n$ бросках\n",
    "\n",
    "**Правдоподобие**\n",
    "\n",
    "Биномиальное распределение \n",
    "\n",
    "$$p(x|q) = C_n^x q^x (1-q)^{n-x}$$\n",
    "\n",
    "**Сопряжённое априорное**\n",
    "\n",
    "Бета распределение \n",
    "\n",
    "$$p(q| \\alpha, \\beta) = \\frac{q^{\\alpha-1} (1-q)^{\\beta-1}}{B(\\alpha, \\beta)}$$\n",
    "\n",
    "Числитель апостериорного распределения:\n",
    "\n",
    "$$ q^{\\alpha-1+x} (1-q)^{\\beta-1+(n-x)} = q^{\\alpha^\\prime-1} (1-q)^{\\beta^\\prime-1},$$\n",
    "\n",
    "где $\\alpha^\\prime = \\alpha + x, \\beta^\\prime = \\beta + n - x$, то есть по сути знаменатель нам тоже известен, он равен $B(\\alpha^\\prime, \\beta^\\prime)$.\n",
    "\n",
    "**Апостериорное распределение**\n",
    "\n",
    "$$ p(q|x) = \\frac{q^{\\alpha-1+x} (1-q)^{\\beta-1+(n-x)}}{B(\\alpha + x, \\beta + n - x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov chain Monte Carlo\n",
    "\n",
    "**Мотивация**\n",
    "\n",
    "Хотелось бы уметь считать математические ожидания и прочее по апостериорному распределению, известному с точностью до константы.\n",
    "\n",
    "Например, хотелось бы узнать ответ ансамбля, состоящего из бесконечного количества алгоритмов :)\n",
    "\n",
    "Мы можем суммировать (интегрировать) матожидания ответа классификатора по вероятности параметров модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод Монте-Карло\n",
    "\n",
    "широкий класс вычислительных алгоритмов, в основе которых лежит многократное повторение случайных экспериментов и последующий расчет интересующих величин\n",
    "\n",
    " * придуман Станислав Улам, когда он во время болезни раскладывал пасьянс и задался вопросом, какова вероятность разложить все карты? Решил не комбинаторно, а просто по доле успешных игр\n",
    " \n",
    " * метод назван в честь казино Монте-Карло в Монако для секретности, т.к. использовался в Манхэттенском проекте США по разработке ядерного оружия\n",
    "\n",
    "$$ \\int f(x)dx \\approx \\frac{1}{n} \\sum\\limits_{i=1}^n f(x_i)$$\n",
    "\n",
    "\n",
    "----\n",
    "Николас Метрополис, Станислав Улам, 1949\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пример использования метода Монте-Карло\n",
    "\n",
    "$$ \\pi = 4 E[x^2 + y^2 < 1] \\approx \\frac{1}{n} \\sum\\limits_{i=1}^n [x_i^2 + y_i^2 < 1], \\\\\n",
    "x_i, y_i \\sim U(0, 1)\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/monte_carlo.jpg\" alt=\"monte_carlo\" width=200 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Марковские цепи\n",
    "\n",
    "Последовательность $x_1, x_2, \\dots, x_{n-1}, x_n$ — *цепь Маркова*, если\n",
    "\n",
    "$$p(x_1, x_2, \\dots, x_{n-1}, x_n) = p(x_n|x_{n-1}) \\dots p(x_2|x_1)p(x_1)$$\n",
    "\n",
    " * Марковские цепи используют для моделирования многих вероятностных распределений\n",
    " * то есть можно так построить процесс, что последовательность $x$ будет как будто сгенерирована из вероятностного распределения\n",
    "\n",
    "----\n",
    "Андрей Андреевич Марков (старший), 1906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MCMC в подсчёте математических ожиданий\n",
    "\n",
    "И в этом случае можем методом Монте-Карло считать мат.ожидания по этому вероятностному распределению\n",
    "\n",
    "$$ E_{p(x)} f(x) \\approx \\frac{1}{n} \\sum\\limits_{i=1}^n f(x_i), \\\\\n",
    "\\text{где } x_i \\sim p(x)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 2:</b> Как моделировать распределения с помощью цепей Маркова?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Сэмплирование по Гиббсу\n",
    "\n",
    "Частный случай алгоритма Метрополиса-Гастингса, наван в честь выдающегося ученого Джозайи Гиббса (\n",
    "https://ru.wikipedia.org/wiki/Семплирование_по_Гиббсу)\n",
    "\n",
    " * допустим нашли апостериорное распределение только с точностью до нормирующей константы\n",
    "\n",
    " $$p(x_1, x_2, x_3) = \\frac{\\hat p(x_1, x_2, x_3)}{C}$$\n",
    "\n",
    "Начинаем с $(x_1^0, x_2^0, x_3^0)$ и в цикле делаем следующее:\n",
    "\n",
    "$$ x_1^1 \\sim p(x_1 | x_2 = x_2^0, x_3 = x_3^0) \\\\\n",
    "   x_2^1 \\sim p(x_2 | x_1 = x_1^1, x_3 = x_3^0) \\\\\n",
    "   x_3^1 \\sim p(x_3 | x_1 = x_1^1, x_2 = x_2^1) \\\\\n",
    "   x_1^2 \\sim p(x_1 | x_2 = x_2^1, x_3 = x_3^1) \\\\\n",
    "   \\dots\n",
    "$$\n",
    "\n",
    " * и через несколько итераций «после разогрева цепи» по этим семплам можно считать мат.ожидания\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Сэмплирование из одномерного распределения\n",
    "\n",
    "**Rejection Sampling**\n",
    "\n",
    "$i = 0$\n",
    "\n",
    "Повторяем много раз:\n",
    "\n",
    "$\\ \\ \\ $ сэмплируем $y \\sim q(y)$\n",
    "\n",
    "$\\ \\ \\ $ сэмплируем $\\xi \\sim U[0;Cq(y)]$\n",
    "\n",
    "$\\ \\ \\ $ **если** $\\xi < \\hat p(y)$:\n",
    "\n",
    "$\\ \\ \\ $ $\\ \\ \\ $ $x_i = y$\n",
    "\n",
    "$\\ \\ \\ $ $\\ \\ \\ $ $i = i+1$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/sampling_1D.jpg\" alt=\"sampling_1D\" width=400 />\n",
    "</div>\n",
    "\n",
    "Трудность в подборе параметра $C$, часто бывает слишком большой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Резюме\n",
    "\n",
    " * Частотный подход — передельный случай байесовского подхода\n",
    " * Байесовский подход обладает массой хороших свойств:\n",
    "   - Регуляризация\n",
    "   - Композитность\n",
    "   - Обработка данных «на лету»\n",
    "   - Можем считать различные статистики\n",
    "   - Можем оценивать уверенность\n",
    " * Однако зачастую бывает вычислительно затратен\n",
    " * Можно пользоваться некоторыми плюсами байесовского подхода, не вычисляя нормирующую константу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пример («парадокс» Монти Холла)\n",
    "\n",
    " * Шоу — дано 3 двери, за одной из которых спрятан приз\n",
    " * Нужно выбрать дверь\n",
    " * После нашего выбора, ведущий открывает одну из не выбранных дверей, за которой точно нет приза (он знает заранее, где приз). И предлагает нам изменить наш выбор\n",
    " \n",
    "<div align=\"center\">\n",
    "    <img src=\"images/paradoks-monti-holla.png\" alt=\"paradoks-monti-holla\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 1:</b> Что выгоднее — поменять выбор или оставить прежним?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пример (\"парадокс\" Монти Холла)\n",
    "\n",
    " * пусть $x_i = 1$ значит, что приз за $i$-ой дверью\n",
    " * $x_i = 0$ значит, что НЕТ приза за $i$-ой дверью\n",
    " * $y = i$ значит, что ведущий открыл $i$-ую дверь\n",
    " \n",
    "$$p(x_1 = 1 | y = 2) = \\frac{p(x_1 = 1, y = 2)}{p(x_1 = 1, y = 2) + p(x_1 = 0, y = 2)} = \\\\\n",
    "  = \\frac{p(x_1 = 1, y = 2)}{p(x_1 = 1, y = 2) + p(x_2 = 1 | y = 2) + p(x_3 = 1 | y = 2)} = \\frac{1/6}{1/6 + 1/6 + 1/6} = \\frac13$$\n",
    "  \n",
    "$$p(x_1 = 0 | y = 2) = \\frac{p(x_1 = 0, y = 2)}{p(x_1 = 1, y = 2) + p(x_1 = 0, y = 2)} = \\\\\n",
    "  = \\frac{p(x_1 = 0, y = 2)}{p(x_1 = 1, y = 2) + p(x_2 = 1 | y = 2) + p(x_3 = 1 | y = 2)} = \\frac{1/3}{1/6 + 1/6 + 1/6} = \\frac23$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 2:</b> А если дверей больше трёх?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
