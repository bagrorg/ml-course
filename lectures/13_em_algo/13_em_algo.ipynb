{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/logo_fmkn.png\" alt=\"logo_fmkn\" />\n",
    "</div>\n",
    "\n",
    "# Машинное обучение 1\n",
    "\n",
    "### Лекция 13. ЕМ алгоритм\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "10 декабря 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пятиминутка\n",
    "\n",
    "1. Выпишите формулу апостериорной вероятности параметров, иллюстрирующую байесовский подход к задаче машинного обучения\n",
    "2. Выпишите определение сопряжённых распределений\n",
    "3. Перечислите основные достоинства байесовского подхода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/kl_div_mem.jpg\" alt=\"kl_div_mem\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Расстояние Кульбака-Лейблера (KL-дивергенция)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Неравенство Йенсена\n",
    "\n",
    "Если $f(x)$ — вогнутая (выпуклая вверх) функция, то справедливо неравенство Йенсена:\n",
    "\n",
    "$$ f(wx_1 + (1 − w)x_2) \\geq \\color{orange}{wf(x_1) + (1 − w)f(x_2)} $$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/concave_function.jpg\" alt=\"concave_function\" />\n",
    "</div>\n",
    "\n",
    "Либо в более общем случае\n",
    "\n",
    "$$f(w_1x_1 + w_2x_2 + \\dots + w_nx_n) \\geq w_1f(x_1) + w_2f(x_2) + \\dots + w_nf(x_n)$$\n",
    "\n",
    "где $ \\sum\\limits_{i=1}^n w_i = 1$\n",
    "\n",
    "Можно доказать, что справедливо и\n",
    "\n",
    "$$  f\\left(\\mathbb{E}_{p(x)} u(x)\\right) \\geq \\mathbb{E}_{p(x)} f(u(x)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Расстояние Кульбака-Лейблера между вероятностными распределениями\n",
    "\n",
    "$$ KL(p||q) = \\int p(x) log \\frac{p(x)}{q(x)} dx $$\n",
    "\n",
    "Свойства дивергенции Кульбака-Лейблера:\n",
    " * $KL(p||q) \\neq KL(q||p)$ — несимметричность\n",
    " * $KL(p||q) \\geq 0$ — неотрицательность\n",
    " * $KL(p||q) = 0 \\Leftrightarrow q(x) = p(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Доказательство неотрицательности KL-дивергенции\n",
    "\n",
    "$$ -KL(p||q) = -\\int p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right)dx = \\int p(x) \\log\\left(\\frac{q(x)}{p(x)}\\right)dx \\leq \\\\\n",
    "   \\leq \\log\\left( \\int p(x) \\frac{q(x)}{p(x)}dx\\right) = \\log\\left( \\int q(x) dx\\right) = \\log 1 = 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Несимметричность KL-дивергенции. Пример: поиск похожего распределения.\n",
    "\n",
    "$ KL(? || ?) $ — на какое место поставить $p$?\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/example.jpg\" alt=\"example_1\" width=400 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Несимметричность KL-дивергенции. Первым аргументом: mass covering\n",
    "\n",
    "$ KL(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/example_1.jpg\" alt=\"example_1\" width=400 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Несимметричность KL-дивергенции. Вторым аргументом: mode collapsing\n",
    "\n",
    "$ KL(q || p) = \\int q(x) \\log \\frac{q(x)}{p(x)} dx$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/example_2.jpg\" alt=\"example_2\" width=400 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Напоминания и экспоненциальное семейство распределений\n",
    "\n",
    " * предполагаем, что в линейной регрессии ответы генерируются из нормального распределения\n",
    " \n",
    " $$ y_i \\sim N(\\mu_i, \\sigma_i^2),\\ \\ \\ \\mu_i = E y_i = x_i^T w$$\n",
    " \n",
    " * Обобщенная линейная модель для регрессии\n",
    "\n",
    " $$ y_i \\sim \\color{red}{Exp}(\\mu_i, \\phi_i),\\ \\ \\ \\mu_i = E y_i,\\ \\ \\ \\color{red}{g(\\mu_i)} = \\theta_i = g(x_i^T w)$$\n",
    "\n",
    " $g(\\mu_i)$ — монотонная функция связи (link function)\n",
    " \n",
    " $Exp$ — экспоненциальное семейство распределений с параметрами $\\theta_i, \\phi_i$ и параметрами-функциями $c(\\theta), h(y, \\phi)$, то есть с плотностью\n",
    " \n",
    " $$ p(y_i | \\theta_i, \\phi_i) = \\exp\\left(\\frac{y_i\\theta_i - c(\\theta_i)}{\\phi_i} + h(y_i, \\phi_i)\\right) \\\\\n",
    "    \\theta_i = g\\left(\\sum\\limits_{j=1}^n w_j f_j(x_i) \\right)\n",
    " $$\n",
    " \n",
    " Обычно удобно работать с семейством распределений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/you-wont-be-disappointed-if-you-have-no-expectation.jpg\" alt=\"exp_mem\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EM-алгоритм: мотивирующий пример\n",
    "\n",
    "Как научиться восстанавливать распределения?\n",
    "\n",
    "Гауссиана:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/gauss_1.jpg\" alt=\"gauss_1\" width=400 />\n",
    "</div>\n",
    "\n",
    "Несколько гауссиан:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/gauss_3.jpg\" alt=\"gauss_1\" width=400 />\n",
    "</div>\n",
    "\n",
    "\n",
    "Неизвестное распределение:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/gauss_unknown.jpg\" alt=\"gauss_unknown\" width=400 />\n",
    "</div>\n",
    "\n",
    "Можем ввести номер гауссианы и свести задачу к уже решенной!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вводим латентные переменные\n",
    "\n",
    "Хотим $p(X|\\theta) \\underset{\\theta}{\\to} \\max$ (неполное правдоподобие)\n",
    "\n",
    "Умеем $p(X, Z|\\theta) \\underset{\\theta}{\\to} \\max$ (полное правдоподобие)\n",
    "\n",
    "Пусть в предыдущем примере $z_i$ — это идентификатор гауссианы, из которой берётся $i$-ый объект.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вариационная нижняя граница\n",
    "\n",
    "Для одного наблюдения $x$:\n",
    "\n",
    "$$ \\log p(x|\\theta) = \\int q(z) \\log p(x|\\theta) dz = \\int q(z) \\log \\frac{p(x, z|\\theta)}{p(z|x,\\theta)} dz = \\\\ \n",
    "   \\int q(z) \\log \\frac{p(x, z|\\theta)q(z)}{p(z|x,\\theta)q(z)} dz = \n",
    "   \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} dz + \\int q(z) \\log \\frac{q(z)}{p(z|x,\\theta)} dz \\geq \\\\\n",
    "   \\geq \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} dz = \\mathscr{L}(q, \\theta)\n",
    "$$\n",
    "— вариационная нижняя граница (просто отбросили неотрицательное $KL(q(z) || p(z|x,\\theta))$)\n",
    "\n",
    "Причем равенство достигается при $q^* = p(z| x, \\theta)$\n",
    "\n",
    "Для выборки $X$:\n",
    "\n",
    "$$ \\log p(X| \\theta) = \\sum\\limits_{i=1}^n \\log p(x_i|\\theta) \\geq \\sum\\limits_{i=1}^n \\mathscr{L}(q_i, \\theta) \\underset{q, \\theta}{\\to} \\max$$\n",
    "\n",
    "Максимизировать по $\\theta$ легко, если знаем $q$ и наоборот.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EM-алгоритм\n",
    "\n",
    "E-шаг (Expectation):\n",
    "\n",
    "$$ \\sum\\limits_{i=1}^n \\mathscr{L}(q_i, \\theta) \\underset{q_i}{\\to} \\max \\Leftrightarrow\n",
    " q_i(z_i) = p(z_i|x_i, \\theta)\n",
    "$$\n",
    "\n",
    "M-шаг (Maximization):\n",
    "\n",
    "$$ \\max\\limits_\\theta \\sum\\limits_{i=1}^n \\mathscr{L}(q_i, \\theta) = \n",
    "   \\max\\limits_\\theta \\sum\\limits_{i=1}^n \\int q_i(z_i) \\log\\frac{p(x_i, z_i| \\theta)}{q_i(z_i)} dz_i = \\\\\n",
    "   \\max\\limits_\\theta \\sum\\limits_{i=1}^n E_{q_i(z_i)} \\log p(x_i, z_i| \\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/em.gif\" alt=\"em\" width=600 />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/GMM.jpg\" alt=\"GMM\" width=400 />\n",
    "</div>\n",
    "\n",
    "$$ p(x_i, z_i|\\theta) = p(x_i|z_i,\\theta)p(z_i|\\theta) = N(x_i|\\mu_{z_i}, \\sigma_{z_i}^2) \\pi_{z_i} $$\n",
    "\n",
    "\n",
    "Параметры: $\\{\\mu_j, \\sigma_j, \\pi_j\\}$, здесь $\\pi_j$ — априорная вероятность $j$-ой гауссианы (cкрытая переменная $z_i$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 1:</b> Как найти число компонент смеси?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * можно простым перебором по их количеству (метод model selection)\n",
    " * можно по-другому – ввести априорное распределение **сразу на все модели разной сложности**. Это и есть основная идея байесовских непараметрических методов\n",
    " * см. процесс китайского ресторана и индийского буфета \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EM-алгоритм для Gaussian Mixture Model\n",
    "\n",
    "E-шаг:\n",
    "\n",
    "$$ q_i(z_i{\\Tiny=}j) = p(z_i{\\Tiny=}j|x_i, \\theta) = \\frac{p(x_i|z_i {\\Tiny=} j, \\theta)p(z_i {\\Tiny=} j|\\theta)}{p(x_i|\\theta)} = \\\\\n",
    "   = \\frac{p(x_i|z_i {\\Tiny=} j, \\theta)p(z_i {\\Tiny=} j|\\theta)}{\\sum\\limits_{k=1}^K p(x_i|z_i {\\Tiny=} k, \\theta)p(z_i {\\Tiny=} k|\\theta)}\n",
    "   = \\frac{N(x_i|\\mu_j, \\sigma_j^2) \\pi_j}{\\sum\\limits_{k=1}^K N(x_i|\\mu_k, \\sigma_k^2) \\pi_k}\n",
    "$$\n",
    "\n",
    "M-шаг:\n",
    "\n",
    "$$ \\sum\\limits_{i=1}^n \\mathscr{L}(q,\\theta) = \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K q_i(z_i=k) \\log\\frac{p(x_i|z_i = k, \\theta)p(z_i = k|\\theta)}{q_i(z_i=k)} = \\\\\n",
    "\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K q_i(z_i=k) \\log\\frac{N(x_i|\\mu_k, \\sigma_k^2)\\pi_k}{q_i(z_i=k)}\n",
    "$$\n",
    "\n",
    "Далее как обычно находим нули производной:\n",
    "$$ \\mu_j = \\frac{\\sum\\limits_{i=1}^n q_i(z_i=j)x_i}{\\sum\\limits_{i=1}^n q_i(z_i=j)};\\ \n",
    "   \\sigma_j^2 = \\frac{\\sum\\limits_{i=1}^n q_i(z_i=j)(x_i-\\mu_j)^2}{\\sum\\limits_{i=1}^n q_i(z_i=j)};\\ \n",
    "   \\pi_j = \\frac{1}{n} \\sum\\limits_{i=1}^n q_i(z_i = j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод главных компонент: постановка задачи\n",
    "\n",
    "$f_1(x), \\dots, f_D(x)$ — исходные числовые признаки\n",
    "\n",
    "$g_1(x), \\dots, g_d(x)$ — новые числовые признаки, $d \\leq D$\n",
    "\n",
    "**Требование**: старые признаки должны линейно восстанавливаться по новым как можно точнее на обучающей выборке $X^n$\n",
    "\n",
    "$ \\hat f_j(x) = \\sum\\limits_{s=1}^d g_s(x) u_{js}, j=1,\\dots,D , \\forall x \\in X$\n",
    "\n",
    "$ \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^D (\\hat f_j(x_i) - f_j(x_i))^2 \\to \\min\\limits_{\\{g_s(x_i)\\}, \\{u_{js}\\}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Матричные обозначения\n",
    "\n",
    "Матрицы «объекты-признаки», старая и новая:\n",
    "\n",
    "$F_{n \\times D} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   f_1(x_1) & \\dots & f_D(x_1) \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   f_1(x_n) & \\dots & f_D(x_n)\n",
    "  \\end{array} \\right]\n",
    "\\ \\ \\ \n",
    "G_{n \\times d} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   g_1(x_1) & \\dots & g_d(x_1) \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   g_1(x_n) & \\dots & g_d(x_n)\n",
    "  \\end{array} \\right]\n",
    "$\n",
    "\n",
    "Матрица линейного преобразования новых признаков в старые:\n",
    "\n",
    "$U_{D \\times d} =  \n",
    "\\left[ \\begin{array}{ccc}\n",
    "   u_{11} & \\dots & u_{1d} \\\\\n",
    "     \\dots  & \\dots &   \\dots  \\\\\n",
    "   u_{D1} & \\dots & u_{Dd}\n",
    "  \\end{array} \\right],\n",
    "  \\hat F = GU^T \\underset{\\text{хотим}}{\\approx} F\n",
    "$\n",
    "\n",
    "**Найти**: и новые признаки $G$, и преобразование $U$:\n",
    "\n",
    "$\\sum\\limits_{i=1}^\\ell \\sum\\limits_{j=1}^n (\\hat f_j(x_i) - f_j(x_i))^2 = \\| GU^T - F\\|^2 \\to \\min\\limits_{G,U}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вероятностная интерпретация метода главных компонент\n",
    "\n",
    "$$p(x,z|\\theta) = p(x|z, \\theta)p(z) = N(x|Wz + \\mu, \\sigma^2I) N(z|0, I)$$\n",
    "\n",
    "$x \\in R^D,\\ z \\in R^d,\\ z \\sim N(0, I)$\n",
    "\n",
    "Параметры ${W, \\mu, \\sigma}$\n",
    "\n",
    "$\\mu \\in R^D, W \\in R^{D\\times d}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Замечание</b> На матрицу $W$ тоже можно наложить ограничения, выраженные априорным распределением, чтобы модель минимизировала $d$ при оптимизации.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EM-алгоритм для PPCA\n",
    "\n",
    "E-шаг (считается аналитически, т.к. сопряженные распределения):\n",
    "\n",
    "$$q_i(z_i) = p(z_i|x_i, \\theta) = \\frac{p(x_i|z_i, \\theta)p(z_i)}{\\int p(x_i|z_i, \\theta)p(z_i) dz_i} = \\\\\n",
    "  = N(z_i| (\\sigma^2I + W^TW)^{-1}W^T (x_i - \\mu), (I + \\sigma^{-2}W^TW)^{-1})\n",
    "$$\n",
    "\n",
    "\n",
    "M-шаг:\n",
    "$$E_Z\\log p(X, Z|\\theta) = \\sum\\limits_{i=1}^n E_{z_i} \\log p(x_i, z_i|\\theta) = \\\\\n",
    "  \\sum\\limits_{i=1}^n E_{z_i} \\log p(x_i| z_i, \\theta) + const \\underset{\\theta}{\\to} \\max\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### M-шаг, пример: поиск W\n",
    "\n",
    "$$\\sum\\limits_{i=1}^n E_{z_i} \\log p(x_i| z_i, \\theta) = \\\\\n",
    " = \\sum\\limits_{i=1}^n E_{z_i} \\left[-\\frac{D}{2}\\log2\\pi-D\\log\\sigma-\\frac{1}{2\\sigma^2}\\left((x_i - \\mu)-Wz_i\\right)^T\\left((x_i - \\mu)-Wz_i\\right) \\right] = \\\\\n",
    " = const(\\sigma) - \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n E_{z_i} \\left[(x_i - \\mu)^T(x_i - \\mu) - 2(x_i - \\mu)^T Wz_i + z_i^TW^T Wz_i \\right] = \\\\\n",
    " = const(\\sigma) - \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n \\left[(x_i - \\mu)^T(x_i - \\mu) - 2(x_i - \\mu)^T W E_{z_i}z_i + tr(W^T WE_{z_i}z_iz_i^T) \\right]\n",
    "$$\n",
    "\n",
    "Далее дифференцируем по $W$:\n",
    "\n",
    "$$ - \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n \\left[-2(x_i - \\mu) E_{z_i} z_i^T + 2W E_{z_i} z_i z_i^T \\right] = 0 \\\\\n",
    "  W^* = \\left( \\sum\\limits_{i=1}^n (x_i - \\mu) E_{z_i} z_i^T \\right) \\left( \\sum\\limits_{i=1}^n E_{z_i} z_i z_i^T \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Матожидания мы можем посчитать, т.к. знаем распределение $q$ с предыдущего Е-шага.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Сравнение оценок на время выполнения\n",
    "\n",
    "При $n > D > d$\n",
    "\n",
    "Стандартный PCA: $O(nD^2)$\n",
    "\n",
    "1 итерация PPCA: $O(nDd)$\n",
    "\n",
    "Обычно нужно 10-100 итераций для сходимости, поэтому может быть быстрее стандартного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Случай пропущенных признаков в PPCA\n",
    "\n",
    "$$(X, Z) = X_{\\text{obs}} \\cup X_{\\text{hid}} \\\\\n",
    "   P(X_{\\text{obs}}|\\theta) \\underset{\\theta}{\\to} \\max\n",
    "$$\n",
    "\n",
    "E-шаг\n",
    "\n",
    "$$ q(X_{\\text{hid}}) = p(X_{\\text{hid}}|X_{\\text{obs}}, \\theta) $$\n",
    "\n",
    "\n",
    "M-шаг\n",
    "\n",
    "$$E_{X_\\text{hid}} \\log p(X_{\\text{obs}} , X_{\\text{hid}} |\\theta) \\underset{\\theta}{\\to} \\max $$\n",
    "\n",
    " * ещё получаем подходящие вероятностные распределения для скрытых переменных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Резюме\n",
    "\n",
    " * Частая метрика между вероятностными распределениями — расстояние Кульбака-Лейблера (KL-дивергенция)\n",
    " * ЕМ-алгоритм позволяет итеративно решать запутанные задачи, разделяя их на два шага\n",
    " * Вероятностный метод главных компонент с использованием ЕМ-алгоритма"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
