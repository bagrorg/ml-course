{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/logo_fmkn.png\" alt=\"logo_fmkn\" />\n",
    "</div>\n",
    "\n",
    "# Машинное обучение\n",
    "\n",
    "### Лекция 11. Кластеризация и частичное обучение\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "26 ноября 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Пятиминутка\n",
    "\n",
    "1. Приведите пример логической функции, которую нельзя реализовать одним нейроном\n",
    "2. Нарисуйте графики известных вам функций активации\n",
    "3. Перечислите преимущества метода обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/cluster_mem.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Постановка задачи кластеризации\n",
    "\n",
    "**Дано**:\n",
    " \n",
    " $X$ — пространство объектов\n",
    " \n",
    " $X^\\ell = \\{x_1, \\dots, x_\\ell \\}$ — обучающая выборка\n",
    "\n",
    " $\\rho: X \\times X \\to [0, \\infty)$ — функция расстояния между объектами\n",
    "\n",
    "**Найти**:\n",
    "\n",
    " $Y$ — множество кластеров\n",
    " \n",
    " $a: X \\to Y$ — алгоритм кластеризации\n",
    "\n",
    " такие, что:\n",
    " * каждый кластер состоит из близких объектов\n",
    " * объекты разных кластеров существенно различны \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Это задача **обучения без учителя** (unsupervised learning)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Некорректность задачи кластеризации\n",
    "\n",
    "Решение задачи кластеризации принципиально неоднозначно\n",
    " * точной постановки задачи кластеризации нет\n",
    " * существует много критериев качества кластеризации\n",
    " * существует много эвристических методов кластеризации\n",
    " * число кластеров $|Y|$, как правило, неизвестно заранее\n",
    " * результат кластеризации сильно зависит от метрики $\\rho$, выбор которой также является эвристикой\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 1:</b> Сколько здесь кластеров?\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/cluster_example.jpg\" alt=\"cluster_example\" width=1000 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Цели кластеризации\n",
    "\n",
    " * **Упростить дальнейшую обработку данных**\n",
    "   - разбить множество $X^\\ell$ на группы схожих объектов, чтобы работать с каждой группой в отдельности (задачи классификации, регрессии, прогнозирования)\n",
    " * **Сократить объём хранимых данных**\n",
    "   - оставить по одному представителю от каждого кластера (задачи сжатия данных)\n",
    " * **Выделить нетипичные объекты**\n",
    "   - те, которые не подходят ни к одному из кластеров (задачи одноклассовой классификации)\n",
    " * **Построить иерархию множества объектов**\n",
    "   - задачи таксономии\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Типы кластерных структур\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <td><img src=\"images/type1.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">внутрикластерные расстояния, как правило, меньше межкластерных</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/type2.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">ленточные кластеры</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/type3.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">кластеры с центром</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Типы кластерных структур\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <td><img src=\"images/type4.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">кластеры могут соединяться перемычками</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/type5.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">кластеры могут накладываться на разреженный фон из редко расположенных объектов</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/type6.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">кластеры могут перекрываться</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Типы кластерных структур\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <td><img src=\"images/type7.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">кластеры могут образовываться не по близости, а по иным типам регулярностей</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/type8.jpg\" alt=\"type\" /></td>\n",
    "    <td style=\"text-align:center\">кластеры могут вообще отсутствовать</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    " * каждый метод кластеризации имеет свои ограничения и выделяет кластеры лишь некоторых типов\n",
    " * понятие «тип кластерной структуры» зависит от метода и также не имеет формального определения\n",
    " * в многомерном случае всё это ещё более верно\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Постановка задачи частичного обучения\n",
    "\n",
    "**Дано**:\n",
    " \n",
    " множество объектов $X$, множество классов $Y$\n",
    " \n",
    " $X^k = \n",
    " \\begin{array}{c}\n",
    " \\{x_1, \\dots, x_k \\} \\\\\n",
    " \\color{red}{\\{y_1, \\dots, y_k \\}} \\\\\n",
    " \\end{array}$ — размеченные объекты (labeled data)\n",
    "\n",
    " $U = \\{x_{k+1}, \\dots, x_\\ell \\}$ — неразмеченные объекты (unlabeled data)\n",
    "\n",
    "**Два варианта постановки задачи**:\n",
    " \n",
    " _Частичное обучение_ (semi-supervised learning, SSL): \n",
    " \n",
    " построить алгоритм классификации $a: X \\to Y$\n",
    "\n",
    " _Трансдуктивное обучение_ (transductive learning): \n",
    " \n",
    " зная $\\color{red}{\\text{все}}\\ \\{x_{k+1}, \\dots, x_\\ell \\}$, получить метки $\\{a_{k+1}, \\dots, a_\\ell\\}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Типичные приложения**:\n",
    "\n",
    "классификация и каталогизация текстов, изображений и т.п.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SSL не сводится к классификации\n",
    "\n",
    "\n",
    "**Пример 1**. Плотности классов, восстановленные:\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td style=\"text-align:center\">по размеченным данным $X^k$</td>\n",
    "    <td style=\"text-align:center\">по полным данным $X^\\ell$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/not_only_classification_1.jpg\" alt=\"not_only_classification\" /></td>\n",
    "    <td><img src=\"images/not_only_classification_2.jpg\" alt=\"not_only_classification\" /></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SSL не сводится к классификации\n",
    "\n",
    "\n",
    "**Пример 2**. Методы классификации не учитывают кластерную структуру неразмеченных данных\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/two_moons.jpg\" alt=\"two_moons\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 2:</b> Сводится ли SSL к кластеризации?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Однако и к кластеризации SSL также не сводится\n",
    "\n",
    "**Пример 3**. Методы кластеризации не учитывают приоритетность разметки над кластерной структурой\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/two_moons_2.jpg\" alt=\"two_moons_2\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Критерии качества кластеризации\n",
    "\n",
    "Пусть известны **только** попарные расстояния между объектами.\n",
    "\n",
    "Среднее внутрикластерное расстояние:\n",
    "\n",
    "$F_0 = \\frac{\\sum\\limits_{i<j}[a_i = a_j] \\rho(x_i, x_j)}{\\sum\\limits_{i<j}[a_i = a_j] } \\to \\min$\n",
    "\n",
    "Среднее межкластерное расстояние:\n",
    "\n",
    "$F_1 = \\frac{\\sum\\limits_{i<j}[a_i \\neq a_j] \\rho(x_i, x_j)}{\\sum\\limits_{i<j}[a_i \\neq a_j] } \\to \\max$\n",
    "\n",
    "Отношение пары функционалов:\n",
    "\n",
    "$F_0 / F_1 \\to \\min$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Критерии качества кластеризации в векторном пространстве\n",
    "\n",
    "Пусть объекты $x_i$ задаются векторами $(f_1(x_i),\\dots,f_n(x_i))$\n",
    "\n",
    " * Сумма средних внутрикластерных расстояний:\n",
    "\n",
    "$\\Phi_0 = \\sum\\limits_{a \\in Y} \\frac{1}{|X_a|} \\sum\\limits_{i: a_i = a} \\rho(x_i, \\mu_a) \\to \\min$\n",
    "\n",
    "$X_a = \\{x_i \\in X^\\ell | a_i = a\\}$ — кластер $a$, $\\mu_a$ — центр масс кластера $a$\n",
    "\n",
    " * Сумма межкластерных расстояний:\n",
    "\n",
    "$\\Phi_1 = \\sum\\limits_{a, b \\in Y} \\rho(\\mu_a, \\mu_b) \\to \\max$\n",
    "\n",
    " * Отношение пары функционалов:\n",
    "\n",
    "$\\Phi_0 / \\Phi_1 \\to \\min$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Кластеризация как задача дискретной оптимизации\n",
    "\n",
    "Веса на парах объектов (близости): $w_{ij} = \\exp(-\\beta \\rho(x_i, x_j))$,\n",
    "\n",
    "где $\\rho(x, x^\\prime)$ — расстояние между объектами, $\\beta$ — параметр\n",
    "\n",
    "**Задача кластеризации**: найти метки кластеров $a_i$\n",
    "\n",
    "$\\sum\\limits_{i=1}^\\ell \\sum\\limits_{j=i+1}^\\ell w_{ij} [a_i \\neq a_j] \\to \\min\\limits_{\\{a_i \\in Y\\}}$\n",
    "\n",
    "**Задача частичного обучения**: \n",
    "\n",
    "$\\sum\\limits_{i=1}^\\ell \\sum\\limits_{j=i+1}^\\ell w_{ij} [a_i \\neq a_j] \\color{red}{+ \\lambda \\sum\\limits_{i=1}^k [a_i \\neq y_i]}  \\to \\min\\limits_{\\{a_i \\in Y\\}}$\n",
    "\n",
    "где $\\lambda$ — ещё один параметр.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"images/brace-yourself-kmeans.jpg\" alt=\"brace-yourself-kmeans\" width=800 />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод k-means (k средних) для кластеризации\n",
    "\n",
    "Минимизация суммы квадратов внутриклассовых расстояний:\n",
    "\n",
    "$$\\sum\\limits_{i=1}^\\ell \\|x_i - \\mu_{a_i} \\|^2 \\to \\min\\limits_{\\{a_i\\}, \\{\\mu_a\\}},\\ \\ \\ \n",
    "\\|x_i - \\mu_{a} \\|^2 = \\sum\\limits_{j=1}^n (f_j(x_i) - \\mu_{a_j})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Алгоритм Ллойда**\n",
    "\n",
    "**Вход**: $X^\\ell, K = |Y|$.  **Выход**: центры кластеров $\\mu_a$ и метки $a \\in Y$\n",
    "\n",
    "1. $\\mu_a = $ начальное приближение центров, для всех $a \\in Y$\n",
    "\n",
    "2. **повторять**\n",
    "\n",
    "3. **Первый шаг**: отнести каждый $x_i$ к ближайшему центру:\n",
    "\n",
    " $a_i = \\arg\\min\\limits_{a \\in Y} \\|x_i - \\mu_a \\|, i = 1, \\dots, \\ell$\n",
    "\n",
    "4. **Второй шаг**: вычислить новые положения центров:\n",
    "\n",
    " $ \\mu_{a_j} = \\frac{\\sum\\limits_{i = 1}^\\ell [a_i = a] x_i}{\\sum\\limits_{i = 1}^\\ell [a_i = a]}, a \\in Y$\n",
    "\n",
    "5. **пока** $a_i$ не перестанут изменяться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### k-means++\n",
    "\n",
    "1: случайным равномерным образом из $X$ взять центр $c_1$\n",
    "\n",
    "2: **повторять $k - 1$ раз**\n",
    "\n",
    "3:   берём $x \\in X$ за центр нового кластера с вероятностью $\\frac{D^2(x)}{\\sum\\limits_{x \\in X} D^2(x)}$\n",
    "\n",
    "4:   используем обычный k-means\n",
    "\n",
    "Здесь $D(x)$ — расстояние от $x$ до ближайшего кластера\n",
    "\n",
    "----\n",
    "\n",
    "David Arthur and Sergei Vassilvitskii. k-means++: The Advantages of Careful Seeding, 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Примеры неудачной кластеризации k-means\n",
    "\n",
    "Причина — неудачное начальное приближение и «ленточные» кластеры\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/k-means-1.jpg\" alt=\"k-means-1\" width=800 />\n",
    "    <img src=\"images/k-means-3.jpg\" alt=\"k-means-3\" width=800 />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Модификация k-means для частичного обучения\n",
    "\n",
    "**Модификация алгоритма Ллойда**\n",
    "\n",
    "<span style=\"color:red\">при наличии размеченных объектов $\\{x_1, \\dots, x_k\\}$</span>, при этом $\\color{red}{U}$ — множество неразмеченных объектов\n",
    "\n",
    "**Вход**: $X^\\ell, K = |Y|$. **Выход**: центры кластеров $\\mu_a, a \\in Y$\n",
    "\n",
    "1. $\\mu_a = $ начальное приближение центров, для всех $a \\in Y$\n",
    "\n",
    "2. **повторять**\n",
    "\n",
    "3. **Первый шаг**: отнести каждый $x_i \\color{red}{\\in U}$ к ближайшему центру:\n",
    "\n",
    " $a_i = \\arg\\min\\limits_{a \\in Y} \\|x_i - \\mu_a \\|, i = \\color{red}{k+1}, \\dots, \\ell$\n",
    "\n",
    "4. **Второй шаг**: вычислить новые положения центров:\n",
    "\n",
    " $ \\mu_{a_j} = \\frac{\\sum\\limits_{i = 1}^\\ell [a_i = a] x_i}{\\sum\\limits_{i = 1}^\\ell [a_i = a]}, a \\in Y$\n",
    "\n",
    "5. **пока** $a_i$ не перестанут изменяться\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм каластеризации DBSAN\n",
    "\n",
    "DBSAN — Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "Объект $x \\in U$, его $\\varepsilon$-окрестность $U_\\varepsilon(x) = \\{ u \\in U: \\rho(x,u) \\leq \\varepsilon\\}$\n",
    "\n",
    "Каждый объект может быть одного из трёх типов:\n",
    " * корневой: имеющий плотную окрестность, $|U_\\varepsilon (x)| \\geq m$\n",
    " * граничный: не корневой, но в окрестности корневого \n",
    " * шумовой (выброс): не корневой и не граничный \n",
    " \n",
    "\n",
    "$ $ \n",
    "  \n",
    "\n",
    "----\n",
    "_Ester, Kriegel, Sander, Xu_. A density-based algorithm for discovering clusters in large spatial databases with noise. KDD-1996\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/DBSCAN-density-data.png\" alt=\"DBSCAN\" width=600 />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм каластеризации DBSAN\n",
    "\n",
    "**Вход**: выборка $Х^\\ell = \\{х_1, \\dots, х_\\ell\\}$, параметры $\\varepsilon$ и $m$\n",
    "\n",
    "**Выход**: разбиение выборки на кластеры и шумовые выбросы; $U = Х^\\ell$ — непомеченные, $a = 0$\n",
    "\n",
    "**пока** в выборке есть непомеченные точки, $U \\neq \\emptyset$:\n",
    "\n",
    "  $\\ \\ \\ $ взять случайную точку $x \\in U$\n",
    "  \n",
    "  $\\ \\ \\ $ **если** $|U_\\varepsilon (x)| < m$ **то** пометить $x$ как, возможно, шумовой\n",
    "  \n",
    "  $\\ \\ \\ $ **иначе** \n",
    "\n",
    "  $\\ \\ \\ $ $\\ \\ \\ $ создать новый кластер: $K = U_\\varepsilon(x), a = a + 1$\n",
    "  \n",
    "  $\\ \\ \\ $ $\\ \\ \\ $ **для всех** $x^\\prime \\in K$, не помеченных или шумовых \n",
    "  \n",
    "  $\\ \\ \\ $ $\\ \\ \\ \\ \\ \\ $ **если** $|U_\\varepsilon(x^\\prime)| \\geq m$ **то** $K = K \\cup U_\\varepsilon(x^\\prime)$ \n",
    "  \n",
    "  $\\ \\ \\ $ $\\ \\ \\ \\ \\ \\ $ **иначе** пометить $x^\\prime$ как граничный кластера $K$\n",
    "  \n",
    "  $\\ \\ \\ $ $\\ \\ \\ $ $a_i = a$ для всех $x_i \\in K$\n",
    "  \n",
    "  $\\ \\ \\ $ $\\ \\ \\ $ $U = U/K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Вопрос 3:</b> Чем хорош DBSCAN?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Преимущества алгоритма DBSCAN\n",
    "\n",
    " * быстрая кластеризация больших данных:\n",
    "   - $O(\\ell^2)$ в худшем случае\n",
    "   - $O(\\ell\\ln \\ell)$ при эффективной реализации $U_\\varepsilon(x)$\n",
    " * кластеры произвольной формы (нет центров)\n",
    " * деление объектов на корневые, граничные и шумовые\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/DBSCAN.jpg\" alt=\"DBSCAN\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Агломеративная иерархическая кластеризация\n",
    "\n",
    "Алгоритм иерархической кластеризации (Ланс, Уильямс, 1967):\n",
    "\n",
    "итеративный пересчёт расстояний $R_{UV}$ между кластерами $U, V$\n",
    "\n",
    "1. $С_1 = \\{\\{х_1\\},\\dots, \\{х_\\ell\\}\\}$ — все кластеры 1-элементные\n",
    "\n",
    "   $R_{\\{х_i\\}\\{х_j\\}} = \\rho(x_i, x_j)$ — расстояния между ними\n",
    "   \n",
    "2. **для всех** $t = 2, \\dots, \\ell$ ($t$ — номер итерации): \n",
    "\n",
    "3. найти в $C_{t-1}$ пару кластеров $(U, V)$ с минимальным $R_{UV}$ \n",
    "\n",
    "4. слить их в один кластер: \n",
    "\n",
    "  $W = U \\cup V$ \n",
    "  \n",
    "  $С_t = С_{t-1} \\cup \\{W\\}/\\{U,V\\}$\n",
    "  \n",
    "5. **для всех** $S \\in C_t$\n",
    "\n",
    "6. вычислить $R_{WS}$ по формуле Ланса-Уильямса: \n",
    "\n",
    "  $R_{WS} = \\alpha_U R_{US} + \\alpha_V R_{VS} + \\beta R_{UV} + \\gamma |R_{US} - R_{VS}|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Частные случаи формулы Ланса-Уильямса\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        \n",
    "1. Расстояние ближнего соседа:\n",
    "\n",
    "  $R^{\\text{б}}_{WS} = \\min_{w \\in W, \\in S} \\rho (w,s)$\n",
    "\n",
    "  $\\alpha_{U} = \\alpha_{V} = \\frac12, \\beta = 0, \\gamma = -\\frac12$    \n",
    "    </td>\n",
    "    <td><img src=\"images/R1.jpg\" alt=\"R1\" width=400 /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "  2. Расстояние дальнего соседа:\n",
    "\n",
    "  $R^{\\text{д}}_{WS} = \\max_{w \\in W, \\in S} \\rho (w,s)$\n",
    "\n",
    "  $\\alpha_{U} = \\alpha_{V} = \\frac12, \\beta = 0, \\gamma = \\frac12$\n",
    "    </td>\n",
    "    <td><img src=\"images/R2.jpg\" alt=\"R2\" width=400 /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "  3. Групповое среднее расстояние:\n",
    "\n",
    "  $R^{\\text{г}}_{WS} = \\frac{1}{|W| |S|} \\sum\\limits_{w \\in W} \\sum\\limits_{s \\in S} \\rho(w, s)$\n",
    "\n",
    "  $\\alpha_{U} = \\frac{|U|}{|W|}, \\alpha_{V} = \\frac{|V|}{|W|}, \\beta = \\gamma = 0$\n",
    "    </td>\n",
    "    <td><img src=\"images/R3.jpg\" alt=\"R3\" width=400 /></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Частные случаи формулы Ланса-Уильямса\n",
    "\n",
    "4. Расстояние между центрами:\n",
    "\n",
    "  $R^{\\text{ц}}_{WS} = \\rho^2\\left(\\sum\\limits_{w \\in W} \\frac{w}{|W|}, \\sum\\limits_{s \\in S} \\frac{s}{|S|} \\right)$\n",
    "  \n",
    "  $\\alpha_{U} = \\frac{|U|}{|W|}, \\alpha_{V} = \\frac{|V|}{|W|}, \\beta = -\\alpha_{U}\\alpha_{V} \\gamma = 0$\n",
    "\n",
    "5. Расстояние Уорда:\n",
    "\n",
    "  $R^{\\text{у}}_{WS} = \\color{red}{\\frac{|S||W|}{|S|+|W|}} \\rho^2\\left(\\sum\\limits_{w \\in W} \\frac{w}{|W|}, \\sum\\limits_{s \\in S} \\frac{s}{|S|} \\right)$\n",
    "\n",
    "  $\\alpha_{U} = \\frac{|S|+|U|}{|S|+|W|}, \\alpha_{V} = \\frac{|S|+|V|}{|S|+|W|}, \\beta = \\frac{-|S|}{|S|+|W|}, \\gamma = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Визуализация кластерной структуры\n",
    "\n",
    "1. Расстояние ближнего соседа\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align:center\">Диаграмма вложения</th>\n",
    "    <th style=\"text-align:center\">Дендрограмма</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:center\"><img src=\"images/diag1.jpg\" alt=\"D1\" width=600 /></td>\n",
    "    <td style=\"text-align:center\"><img src=\"images/dendr1.jpg\" alt=\"D1\" width=600 /></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Визуализация кластерной структуры\n",
    "\n",
    "2. Расстояние дальнего соседа\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align:center\">Диаграмма вложения</th>\n",
    "    <th style=\"text-align:center\">Дендрограмма</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/diag2.jpg\" alt=\"D2\" width=600 /></td>\n",
    "    <td><img src=\"images/dendr2.jpg\" alt=\"D2\" width=600 /></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Основные свойства иерархической кластеризации\n",
    "\n",
    "*Монотонность*: дендрограмма не имеет самопересечений, при каждом слиянии расстояние между объединяемыми кластерами только увеличивается: \n",
    "\n",
    "$R_2 \\leq R_3 \\leq \\dots \\leq R_\\ell$\n",
    "\n",
    "*Сжимающее расстояние*: $R_t \\leq \\rho(\\mu_U, \\mu_V), \\forall t$\n",
    "\n",
    "*Растягивающее расстояние*: $R_t \\geq \\rho(\\mu_U, \\mu_V), \\forall t$\n",
    "\n",
    "**Теорема** (Миллиган, 1979) \n",
    "\n",
    "Кластеризация монотонна, если выполняются условия \n",
    "\n",
    "$\\alpha_U \\geq 0, \\alpha_V \\geq 0, \\alpha_U + \\alpha_V + \\beta \\geq 1, \\min\\{\\alpha_U, \\alpha_V\\} + \\gamma \\geq 0$\n",
    "\n",
    "$R^\\text{ц}$ не монотонно; $R^\\text{б}$, $R^\\text{д}$, $R^\\text{г}$, $\\color{red}{R^\\text{у}}$ — монотонны\n",
    "\n",
    "$R^\\text{б}$ — сжимающее; $R^\\text{д}, \\color{red}{R^\\text{у}}$ — растягивающие \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Рекомендации и выводы\n",
    "\n",
    " * рекомендуется пользоваться расстоянием Уорда $R^\\text{у}$\n",
    " * обычно строят несколько вариантов и выбирают лучший визуально по дендрограмме\n",
    " * определение числа кластеров — по максимуму $|R_{t+1} - R_t|$,\n",
    " \n",
    " тогда результирующее множество кластеров $= C_t$\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td><img src=\"images/conc1.jpg\" alt=\"conc\" width=600 /></td>\n",
    "    <td><img src=\"images/conc2.jpg\" alt=\"conc\" width=600 /></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод частичного обучения self-training (1965-1970)\n",
    "\n",
    "Пусть $\\mu: X^k \\to a$ — метод обучения классификации\n",
    "\n",
    "Классификаторы имеют вид $a(x) = \\arg\\max\\limits_{y \\in Y} \\Gamma_y(x)$\n",
    "\n",
    "*Псевдоотступ* — степень уверенности классификации $a_i = a(x_i)$:\n",
    "\n",
    "$M_i(a) = \\Gamma_{a_i}(x_i) - \\max\\limits_{y \\in Y/a_i} \\Gamma_y(x_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Алгоритм self-training** — обёртка (wrapper) над методом $\\mu$:\n",
    "\n",
    "1. $Z = X^k$\n",
    "\n",
    "2. **пока** $|Z| < \\ell$\n",
    "\n",
    "3. $\\ \\ \\  a = \\mu(Z)$\n",
    "\n",
    "4. $\\ \\ \\  \\Delta = \\{x_i \\in U/Z | M_i(a) \\geq \\color{red}{M_0}\\}$\n",
    "\n",
    "5. $\\ \\ \\  a_i = a(x_i)$ для всех $x_i \\in \\Delta$\n",
    "\n",
    "6. $Z = Z \\cup \\Delta$\n",
    "\n",
    "$\\color{red}{M_0}$ можно определять, например, из условия $|\\Delta| = 0.05 |U|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод частичного обучения co-learning (deSa, 1993)\n",
    "\n",
    "Пусть $\\mu_t: X^k \\to a_t$ — разные методы обучения, $t = 1, \\dots, T$\n",
    "\n",
    "**Алгоритм co-learning** — это self-training для композиции — простого голосования базовых алгоритмов $a_1, \\dots, a_T$:\n",
    "\n",
    " $a(x) = \\arg\\max\\limits_{y \\in Y} \\Gamma_y(x),\\ \\Gamma_y(x_i) = \\sum\\limits_{t=1}^T[a_t(x_i) = y]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Метод частичного обучения co-training (Blum, Mitchell, 1998)\n",
    "\n",
    "Пусть $\\mu_1: X^k \\to a_1,\\ \\mu_2: X^k \\to a_2$ — два существенно различных метода обучения, использующих\n",
    "\n",
    " * либо разные наборы признаков\n",
    " * либо разные парадигмы обучения (inductive bias)\n",
    " * либо разные источники данных $X_1^{k_1}, X_2^{k_2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. $Z_1 = X_1^{k_1}$, $Z_2 = X_2^{k_2}$\n",
    "\n",
    "2. **пока** $|Z_1 \\cup Z_2| < l$\n",
    "\n",
    "3. $\\ \\ \\  a_1 = \\mu_1(Z_1), \\Delta_1 = \\{x_i \\in U/Z_1/Z_2 | M_i(a_1) \\geq \\color{red}{M_{01}}\\}$\n",
    "\n",
    "4. $\\ \\ \\  a_i = a_1(x_i)$ для всех $x_i \\in \\Delta_1$\n",
    "\n",
    "5. $Z_2 = Z_2 \\cup \\Delta_1$\n",
    "\n",
    "6. $\\ \\ \\  a_2 = \\mu_2(Z_2), \\Delta_2 = \\{x_i \\in U/Z_1/Z_2 | M_i(a_2) \\geq \\color{red}{M_{02}}\\}$\n",
    "\n",
    "7. $\\ \\ \\  a_i = a_2(x_i)$ для всех $x_i \\in \\Delta_2$\n",
    "\n",
    "8. $Z_1 = Z_1 \\cup \\Delta_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Резюме\n",
    "\n",
    " * Кластеризация — это обучение без учителя, некорректно поставленная задача, существует много оптимизационных и эвристических алгоритмов кластеризации\n",
    " * DBSCAN — популярный быстрый алгоритм кластеризации\n",
    " * Задача semi-supervised learning (SSL) занимает промежуточное положение между классификацией и кластеризацией, но не сводится к ним\n",
    " * Методы кластеризации легко адаптируются к SSL путём введения ограничений (constrained clustering)\n",
    " * Адаптация методов классификации реализуется сложнее, через дополнительную регуляризацию, и как правило, приводит к более эффективным методам\n",
    " * Регуляризация объединяет критерии на размеченных и неразмеченных данных в одну задачу оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Алгоритм Ланга-Уильямса для частного обучения\n",
    "\n",
    "Алгоритм иерархической кластеризации (Ланс, Уильямс, 1967):\n",
    "\n",
    "итеративный пересчёт расстояний $R_{UV}$ между кластерами $U, V$. \n",
    "\n",
    "1. $С_1 = \\{\\{х_1\\},\\dots, \\{х_l\\}\\}$ — все кластеры 1-элементные\n",
    "\n",
    "   $R_{\\{х_i\\}\\{х_j\\}} = \\rho(x_i, x_j)$ — расстояния между ними\n",
    "   \n",
    "2. **для всех** $t = 2, \\dots, l$ ($t$ — номер итерации): \n",
    "\n",
    "3. найти в $C_{t-1}$ пару кластеров $(U, V)$ с минимальным $R_{UV}$ \n",
    "\n",
    "   <span style=\"color:red\">при условии, что в $U \\cup V$ нет объектов с разными метками</span>\n",
    "\n",
    "4. слить их в один кластер: \n",
    "\n",
    "  $W = U \\cup V$ \n",
    "  \n",
    "  $С_t = С_{t-1} \\cup \\{W\\}/\\{U,V\\}$\n",
    "  \n",
    "5. **для всех** $S \\in C_t$\n",
    "\n",
    "6. вычислить $R_{WS}$ по формуле Ланса-Уильямса: \n",
    "\n",
    "  $R_{WS} = \\alpha_U R_{US} + \\alpha_V R_{VS} + \\beta R_{UV} + \\gamma |R_{US} - R_{VS}|$\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
